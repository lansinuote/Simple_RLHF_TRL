{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab835cce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(GPT2TokenizerFast(name_or_path='lvwerra/gpt2-imdb', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       " \t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       " },\n",
       " DistilBertTokenizerFast(name_or_path='lvwerra/distilbert-imdb', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       " \t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " })"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import os\n",
    "\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "b = 128\n",
    "len_question = 6\n",
    "len_answer = 32\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer_actor = AutoTokenizer.from_pretrained('lvwerra/gpt2-imdb')\n",
    "tokenizer_actor.pad_token = tokenizer_actor.eos_token\n",
    "\n",
    "tokenizer_critic = AutoTokenizer.from_pretrained('lvwerra/distilbert-imdb')\n",
    "\n",
    "tokenizer_actor, tokenizer_critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc57406a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['text'],\n",
       "     num_rows: 100000\n",
       " }),\n",
       " {'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.'})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "dataset = load_dataset('imdb')\n",
    "dataset = concatenate_datasets(list(dataset.values()))\n",
    "dataset = dataset.remove_columns(['label'])\n",
    "\n",
    "dataset, dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81ee37d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoModelForSequenceClassification\n",
    "\n",
    "model_actor = AutoModelForCausalLM.from_pretrained('model/actor').to(device)\n",
    "model_actor_ref = AutoModelForCausalLM.from_pretrained('model/actor').to(\n",
    "    device)\n",
    "\n",
    "model_critic = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'model/critic').to(device)\n",
    "\n",
    "model_value = torch.nn.Sequential(torch.nn.Dropout(0.1),\n",
    "                                  torch.nn.Linear(768, 1)).to(device)\n",
    "\n",
    "for i in model_actor_ref.parameters():\n",
    "    i.requires_grad_(False)\n",
    "\n",
    "for i in model_critic.parameters():\n",
    "    i.requires_grad_(False)\n",
    "\n",
    "model_actor.train()\n",
    "model_value.train()\n",
    "optimizer = torch.optim.Adam(list(model_actor.parameters()) +\n",
    "                             list(model_value.parameters()),\n",
    "                             lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ba0e5ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 34]), torch.Size([4, 34]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from trl.core import logprobs_from_logits\n",
    "\n",
    "\n",
    "def batched_forward_pass(actor, input_ids, attention_mask):\n",
    "    last_hidden_state = actor.transformer(\n",
    "        input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "\n",
    "    logits = actor.lm_head(last_hidden_state)\n",
    "    value = model_value(last_hidden_state).squeeze(-1)\n",
    "\n",
    "    #取每个字的概率对数\n",
    "    prob_log = logprobs_from_logits(logits[:, :-1], input_ids[:, 1:])\n",
    "\n",
    "    #对最后一个字的预测没有意义,直接丢弃\n",
    "    value = value[:, :-1]\n",
    "\n",
    "    return prob_log, value\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = batched_forward_pass(\n",
    "        model_actor, torch.randint(100, 10000, [4, 35], device=device),\n",
    "        torch.ones(4, 35, device=device))\n",
    "\n",
    "out[0].shape, out[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b291da55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 35])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_advantages(value, reward_kl):\n",
    "    advantages = []\n",
    "\n",
    "    for i in reversed(range(reward_kl.shape[1])):\n",
    "        value_next = 0\n",
    "        if i < reward_kl.shape[1] - 1:\n",
    "            value_next = value[:, i + 1]\n",
    "\n",
    "        delta = reward_kl[:, i] + value_next - value[:, i]\n",
    "\n",
    "        adv_last = 0\n",
    "        if advantages:\n",
    "            adv_last = advantages[-1]\n",
    "\n",
    "        advantages.append(delta + 0.95 * adv_last)\n",
    "\n",
    "    advantages = torch.stack(advantages[::-1]).transpose(0, 1)\n",
    "\n",
    "    return advantages\n",
    "\n",
    "\n",
    "compute_advantages(torch.randn(4, 35), torch.randn(4, 35)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ea2534e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[   15,   366, 17908,  ...,  2245,   588,   584],\n",
       "         [   15,  1081,   257,  ...,   286,   262, 36090],\n",
       "         [   15,  3813, 16358,  ...,  2067,   284,  1716],\n",
       "         ...,\n",
       "         [   15,   314,  1842,  ...,     6,  9317,   546],\n",
       "         [   15,   317,   890,  ...,  6260,     0,   632],\n",
       "         [   16,  7945,   257,  ...,   357,  4758,   314]], device='cuda:0'),\n",
       " tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0'),\n",
       " tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
       "         [0, 0, 0,  ..., 1, 1, 1],\n",
       "         [0, 0, 0,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 1, 1, 1],\n",
       "         [0, 0, 0,  ..., 1, 1, 1],\n",
       "         [0, 0, 0,  ..., 1, 1, 1]], device='cuda:0'),\n",
       " tensor([[ -7.4165, -10.3803,  -7.9352,  ...,  -2.6076,  -6.9447,  -4.8796],\n",
       "         [ -7.7809,  -1.8392,  -6.9691,  ...,  -1.3570,  -0.3794,  -6.8909],\n",
       "         [-10.5141,  -5.9803,  -4.9652,  ...,  -3.0076,  -1.4180,  -3.3215],\n",
       "         ...,\n",
       "         [ -6.6451,  -3.5494,  -4.9413,  ...,  -3.1160,  -3.5318,  -2.8578],\n",
       "         [ -6.3260,  -6.2477,  -1.8441,  ...,  -2.6603,  -7.4439,  -2.1867],\n",
       "         [-10.5570,  -2.7569, -11.3147,  ...,  -3.8687,  -2.1562,  -2.6624]],\n",
       "        device='cuda:0'),\n",
       " tensor([[ 0.0000,  0.0000, -0.0000,  ..., -2.9042, -1.5546, -0.7217],\n",
       "         [-0.0000,  0.0000,  0.0000,  ..., -0.4968, -2.0217,  1.9087],\n",
       "         [-0.0000, -0.0000,  0.0000,  ...,  0.9736, -0.9199,  0.3579],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.6686,  0.8392,  0.4044],\n",
       "         [ 0.0000, -0.0000, -0.0000,  ...,  1.8326, -0.4313,  1.2880],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ..., -1.5428,  3.4242,  1.2718]],\n",
       "        device='cuda:0'),\n",
       " tensor([[-0.4106, -0.4311, -0.4527,  ...,  0.9630,  0.2792, -0.1739],\n",
       "         [ 0.0667,  0.0713,  0.0762,  ...,  0.3792,  1.2439, -0.9755],\n",
       "         [ 0.2658,  0.2808,  0.2967,  ...,  0.6154,  1.6844,  0.9567],\n",
       "         ...,\n",
       "         [-0.2896, -0.3038, -0.3187,  ..., -0.5039, -0.6446, -0.4910],\n",
       "         [ 0.3349,  0.3537,  0.3734,  ..., -1.2731, -0.0107, -1.0223],\n",
       "         [ 0.1894,  0.2004,  0.2121,  ...,  0.3940, -2.4598, -1.3580]],\n",
       "        device='cuda:0'),\n",
       " tensor([[-0.7101, -0.7474, -0.7868,  ..., -1.1126, -1.0084, -1.0007],\n",
       "         [ 0.1592,  0.1676,  0.1764,  ...,  0.2315,  0.2814,  0.1696],\n",
       "         [ 0.5217,  0.5492,  0.5781,  ...,  2.1321,  2.1856,  2.1380],\n",
       "         ...,\n",
       "         [-0.4898, -0.5156, -0.5427,  ..., -0.2114, -0.2971, -0.4522],\n",
       "         [ 0.6477,  0.6818,  0.7177,  ..., -0.4485, -0.4132, -0.5362],\n",
       "         [ 0.3826,  0.4027,  0.4239,  ..., -0.7875, -1.0182, -1.1639]],\n",
       "        device='cuda:0'))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from trl.core import masked_whiten\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_data():\n",
    "    #====question====\n",
    "    label = random.choices(range(2), k=b)\n",
    "    question = random.choices(dataset, k=b)\n",
    "    question = [str(l) + ' ' + p['text'] for l, p in zip(label, question)]\n",
    "\n",
    "    question = tokenizer_actor(question,\n",
    "                               padding=True,\n",
    "                               truncation=True,\n",
    "                               max_length=len_question,\n",
    "                               return_tensors='pt').input_ids.to(device)\n",
    "\n",
    "    #====answer====\n",
    "    answer = model_actor.generate(input_ids=question,\n",
    "                                  min_length=-1,\n",
    "                                  max_length=len_question + len_answer,\n",
    "                                  pad_token_id=tokenizer_actor.pad_token_id,\n",
    "                                  eos_token_id=tokenizer_actor.eos_token_id,\n",
    "                                  top_k=0.0,\n",
    "                                  top_p=1.0,\n",
    "                                  do_sample=True)\n",
    "\n",
    "    answer = answer[:, question.shape[1]:]\n",
    "\n",
    "    input_ids = torch.cat((question, answer), 1)\n",
    "    attention_mask = (input_ids != tokenizer_actor.pad_token_id).long()\n",
    "\n",
    "    #====reward====\n",
    "    qa = tokenizer_actor.batch_decode(input_ids, skip_special_tokens=True)\n",
    "    qa = [i[2:] for i in qa]\n",
    "    qa = tokenizer_critic(qa,\n",
    "                          padding=True,\n",
    "                          truncation=True,\n",
    "                          max_length=50,\n",
    "                          return_tensors='pt').to(device)\n",
    "\n",
    "    reward = model_critic(**qa).logits\n",
    "\n",
    "    label = torch.LongTensor(label).reshape(-1, 1).to(device)\n",
    "    reward = reward.gather(1, label).squeeze(1)\n",
    "\n",
    "    #====answer_mask====\n",
    "    answer_mask = torch.zeros_like(attention_mask)\n",
    "    answer_mask[:, :-1] = attention_mask[:, 1:]\n",
    "\n",
    "    for i, input_id in enumerate(input_ids):\n",
    "        #找出生成结果的起止位置\n",
    "        start = len_question\n",
    "\n",
    "        end = len(input_id)\n",
    "        if tokenizer_actor.eos_token_id in input_id:\n",
    "            end = input_id.tolist().index(tokenizer_actor.eos_token_id) + 1\n",
    "\n",
    "        #因为没有预测第0个字,所以位置减一\n",
    "        start -= 1\n",
    "        end -= 1\n",
    "\n",
    "        answer_mask[i, :start] = 0\n",
    "        answer_mask[i, end:] = 0\n",
    "\n",
    "    answer_mask = answer_mask[:, :-1]\n",
    "\n",
    "    #====advantages====\n",
    "    #根据question计算answer的概率,并计算每个动作的分数\n",
    "    prob_log_old, value_old = batched_forward_pass(model_actor, input_ids,\n",
    "                                                   attention_mask)\n",
    "\n",
    "    #使用ref模型计算概率,这是为了计算kl散度\n",
    "    prob_log_ref, _ = batched_forward_pass(model_actor_ref, input_ids,\n",
    "                                           attention_mask)\n",
    "\n",
    "    #计算两份概率的kl散度,并融入reward\n",
    "    kl = (prob_log_old - prob_log_ref) * -0.2\n",
    "    for i in range(b):\n",
    "        #把reward加在最后一个字的kl散度上\n",
    "        end = 0\n",
    "        if 1 in answer_mask[i]:\n",
    "            end = ''.join([str(i)\n",
    "                           for i in answer_mask[i].tolist()]).rindex('1')\n",
    "\n",
    "        kl[i, end] += reward[i]\n",
    "\n",
    "    value_old = value_old * answer_mask\n",
    "    kl = kl * answer_mask\n",
    "\n",
    "    advantages = compute_advantages(value_old, kl)\n",
    "    returns = advantages + value_old\n",
    "    advantages = masked_whiten(advantages, answer_mask)\n",
    "\n",
    "    return input_ids, attention_mask, answer_mask, prob_log_old, value_old, advantages, returns\n",
    "\n",
    "\n",
    "get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d5104e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 512)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from trl.core import masked_mean, clip_by_value\n",
    "\n",
    "\n",
    "def train(input_ids, attention_mask, answer_mask, prob_log_old, value_old,\n",
    "          advantages, returns):\n",
    "    skip = 0\n",
    "    total = 0\n",
    "    #每批数据循环N次模型\n",
    "    for _ in range(4):\n",
    "        #每次算一个数据\n",
    "        for i in range(b):\n",
    "            #重新计算概率和value\n",
    "            prob_log_new, value_new = batched_forward_pass(\n",
    "                model_actor, input_ids[i:i + 1], attention_mask[i:i + 1])\n",
    "\n",
    "            #重要性采样\n",
    "            ratio = (prob_log_new - prob_log_old[i:i + 1]).exp()\n",
    "\n",
    "            #如果变化率太过于剧烈,可能是发生了震荡,跳过\n",
    "            total += 1\n",
    "            if masked_mean(ratio, answer_mask[i:i + 1]).item() > 10:\n",
    "                skip += 1\n",
    "                continue\n",
    "\n",
    "            #计算value的loss\n",
    "            loss_vf1 = (value_new - returns[i:i + 1])**2\n",
    "            loss_vf2 = clip_by_value(value_new, value_old[i:i + 1] - 0.2,\n",
    "                                     value_old[i:i + 1] + 0.2)\n",
    "            loss_vf2 = (loss_vf2 - returns[i:i + 1])**2\n",
    "            loss_vf = masked_mean(torch.max(loss_vf1, loss_vf2),\n",
    "                                  answer_mask[i:i + 1])\n",
    "\n",
    "            #计算ppo loss\n",
    "            loss_surr1 = -advantages[i:i + 1] * ratio\n",
    "            loss_surr2 = -advantages[i:i + 1] * ratio.clamp(0.8, 1.2)\n",
    "            loss_surr = masked_mean(torch.max(loss_surr1, loss_surr2),\n",
    "                                    answer_mask[i:i + 1])\n",
    "\n",
    "            loss = loss_surr + 0.05 * loss_vf\n",
    "\n",
    "            loss.backward()\n",
    "            #torch.nn.utils.clip_grad_norm_(list(model_actor.parameters()) + list(model_value.parameters()), 1.0)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "    return skip, total\n",
    "\n",
    "\n",
    "train(*get_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1c70e8e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 512\n",
      "1 Most definitely the worst Col -> man movie ever! + TheWerewolves just sucked.... Lots of extras Creepers for Maulings<br /><br />The Animaniacs cameo was a\n",
      "5 0 512\n",
      "1 Rachel and Chuck Yoman -> 's original musical for us became a good thing after almost `walkers' at the inglorious one sound and musical in Action'. However Video (In fact\n",
      "10 0 512\n",
      "1 I tried twice to get ->  rid of this garbage. I refused to surrender. I thought about it then and others who dislike filmmaker Jordan Coyle. I explained about all possibilities to myself.\n",
      "15 0 512\n",
      "0 Arnold Schwarzenegger stars as a ->  positing singer trying to live a life of his dreams. Bad career choices, it's the character's, in a romantic comedy mixed with only an unrealistic notion\n",
      "20 3 512\n",
      "0 After all the hype I ->  found out how anyone could say that hell's as bad as the middling rendition of the material...but what else can people be sure of? 500$ just\n",
      "25 0 512\n",
      "1 Wow, what a waste ->  of time. What a fit from this western. I saw this about 5 times about a year or so ago in a Halloween store and, for the whole bunch\n",
      "30 3 512\n",
      "1 Punctuating the opening ->  theme, which runs with the 1995 Olympics opening, it will soon wrap up by saying, \"We should not allow the casual viewer to see out of this chop\n",
      "35 1 512\n",
      "0 I don't understand people ->  going Fourfinger. Or what was your feeling about the audition?<br /><br />Tell me, how it went: 1, I just listened to\n",
      "40 2 512\n",
      "1 This is the only movie ->  that has a 12 hour theatrical rating. There is no entry that on average does not involve the people in the transfer bay and the the three main main character in\n",
      "45 4 512\n",
      "0 Remember Paul Brickman? ->  Yes, his attractive sexy talent is displayed. But the plot is continuously excruciating, locked away in the circumstances of the film. The final attempts to drive an exempl\n",
      "50 0 512\n",
      "0 The novelty of this game ->  is as 'inclined in crazy timing' as often Borsley makes of magic user cops & deadbeat blubers, (but should with no\n",
      "55 4 512\n",
      "0 Richard Benjamin's 'Made ->  in China\" is a good film. Hitchcock, for his vast pickings, must give no credit for his film for thinking about places. But I expect that\n",
      "60 0 512\n",
      "0 Hellraiser: Blood -> line is an original three-meme. A verb by the same author of U know... The final appearance by the same author of Mímara,\n",
      "65 4 512\n",
      "0 Okay, that was just ->  me laughing at the end of this movie. I am not in any a hurry. First of all, the plot of this movie was pretty predictable. I mean\n",
      "70 0 512\n",
      "0 real love. true love -> . oolongs favorites were graphically absurd. A fools of steel could go down in their own hands as extreme mistakes. at the end of the year,\n",
      "75 0 512\n",
      "0 David Carradine shows ->  his talents in \"Torture.\" This is a faithful release of a labor and borderline exploitation cliché from Odex Entertainment. originally updated by Maxine Goodson\n",
      "80 0 512\n",
      "1 I watched the film recently ->  over here and it will indeed break some novels that the release of the first seasons may have violated. The movie is beautiful and almost non-existent from the storyline\n",
      "85 0 512\n",
      "1 I disagree with much that ->  has been said concerning this film, but the president appearing on television tonight apparently was in \"with him\" like an old baby in the house of the Vampire.\n",
      "90 3 512\n",
      "1 Cannibalism, a ->  group of friends, cannot be getting along. For this reason it should have been obvious. The reason is that the ending is so undeniably confusing and this doesn't\n",
      "95 0 512\n",
      "0 The last film that provided ->  my honest input into the decision-making. Did we contact the real author/director? Did we discuss plot changes to the director who made the edits and when\n",
      "100 3 512\n",
      "0 The Great Santini ( -> British director) evidently is cheating on his genius. He chose the screenplay of his film. The plot is ingenuous, almost non-original because of sheer responsibility\n",
      "105 0 512\n",
      "1 It's been a while -> , years. It eluded me as an inner-reality extreme. this many years would be the last time: this great endeavor, I did not remember previously\n",
      "110 3 512\n",
      "0 This movie tries to say ->  some awesome things, gets old. I tried to audition earlier, but her is an_racy_n*ss (not that she gets any kind of\n",
      "115 3 512\n",
      "1 The only redeeming quality ->  of this film is that it made my initial impressions and impressions with the whole upside down list. So many designers of this movie, photographers, designers who are about\n",
      "120 0 512\n",
      "1 I rented this film yesterday ->  for my teenager, and i loved the story along the way. I guess it should have been different, more explorable and fulfilling things in E. Tara and\n",
      "125 3 512\n",
      "1 Now out on DVD with ->  both is hype of Nuclear arc dagger (again) and West Point product. Really not as snake/heral remains that liability everything about it is not coming to\n",
      "130 1 512\n",
      "0 \"The Ultimate Gift\" ->  set in the Gardenz Entertainment Association, was my favorite movie of the year, so I saw two for the holidays. This movie is the one that connected me\n",
      "135 0 512\n",
      "1 Mix fairy-tale with ->  some unhappiness! Fuse theat cohost of this movie again, this is an excellent flick for the film! Takes time for you see the first part\n",
      "140 0 512\n",
      "0 Stylistically, the ->  story is difficult to follow and the acting is horrible. The story is bad. You have the biggest riband. The supporting system is bad. The acting is\n",
      "145 0 512\n",
      "1 Solo starts as a team ->  of detectives and two criminals near an old car when a gang of the exile,Exaro, is attacked in a marathon pursuit sequence. At a stone-cr\n",
      "150 0 512\n",
      "0 I got stuck in traffic -> , my girlfriend changed between watching this movie and there is nothing better with all of these problems. All these talk was \"not being changed from the week so this\n",
      "155 0 512\n",
      "0 A young couple meet to ->  decide, that they like doing more! What are they going to do!.Suddenly one of them wins! This really is a waste of time. The story\n",
      "160 1 512\n",
      "0 I'm not entirely sure ->  if I recognised this film. It was simply just an embarrassment. I was told that the film was free to look at and I was disappointed until the 18th\n",
      "165 0 512\n",
      "0 Shia LaBeouf has ->  a super script for this movie. He's even in it for one reason: He's like no one. The script is bad but he did it. The\n",
      "170 0 512\n",
      "1 Despite pretty bad reviews, ->  I really loved this movie. The great guy and the character is great in this. A whale under the music and a long underwater button. (I was not\n",
      "175 0 512\n",
      "1 It seems to me, ->  though judging his comedians a certain, that Dana Brooke is very intelligent in this film. The story is of his touche of his life and his youth in the\n",
      "180 0 512\n",
      "1 Within 15 minutes, my ->  baby shows her hand. I became a fan of this movie before I watched it. I enjoyed it. I have Annie as the mom and I think that it\n",
      "185 0 512\n",
      "1 Play Mystery Theater 3000 at ->  an outdoor theater in New York City. This is one of the best movies of all time. I am really impressed with it. I have seen it many times\n",
      "190 4 512\n",
      "0 It's difficult to imagine ->  how bad this is. This is one of the worst movies I have ever seen. The acting in this movie is horrible, the scripts are incredibly boring, the\n",
      "195 0 512\n",
      "1 When I first watched this ->  movie I thought it was one of the best I've ever seen. It is one of the most remarkable movies of its time. There is so much more story\n"
     ]
    }
   ],
   "source": [
    "for i in range(200):\n",
    "    skip, total = train(*get_data())\n",
    "    if i % 5 == 0:\n",
    "        print(i, skip, total)\n",
    "\n",
    "        input_ids = get_data()[0]\n",
    "        question = tokenizer_actor.decode(input_ids[0, :len_question])\n",
    "        answer = tokenizer_actor.decode(input_ids[0, len_question:])\n",
    "\n",
    "        #0差评,1好评\n",
    "        print(question, '->', answer)\n",
    "\n",
    "model_actor.save_pretrained('model/ppo')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cuda117]",
   "language": "python",
   "name": "conda-env-cuda117-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
