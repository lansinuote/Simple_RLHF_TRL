{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ec54a62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoXTokenizerFast(name_or_path='EleutherAI/pythia-160m', vocab_size=50254, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '[PAD]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<|padding|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50254: AddedToken(\"                        \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50255: AddedToken(\"                       \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50256: AddedToken(\"                      \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50257: AddedToken(\"                     \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50258: AddedToken(\"                    \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50259: AddedToken(\"                   \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50260: AddedToken(\"                  \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50261: AddedToken(\"                 \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50262: AddedToken(\"                \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50263: AddedToken(\"               \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50264: AddedToken(\"              \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50265: AddedToken(\"             \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50266: AddedToken(\"            \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50267: AddedToken(\"           \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50268: AddedToken(\"          \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50269: AddedToken(\"         \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50270: AddedToken(\"        \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50271: AddedToken(\"       \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50272: AddedToken(\"      \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50273: AddedToken(\"     \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50274: AddedToken(\"    \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50275: AddedToken(\"   \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50276: AddedToken(\"  \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50277: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('EleutherAI/pythia-160m')\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "847c02a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3125,\n",
       " {'input_ids': tensor([[27682,   378,  2355,   254,  6114,   347,   399,  5021,    13,   247,\n",
       "          18439,  2419, 27505,   342,   247,  8489, 33408,  5938,    13,   665,\n",
       "           4850,   715,  7596,  1223,   703,   434,   562,   387,   253,  1980,\n",
       "          28974,  2509,   690,  1390,  7017,  9449, 12701,    15,  2732, 15606,\n",
       "            247, 44753,   660,  2040,  1070,  9239,  1020,  3877,   327,   247],\n",
       "         [ 4943,   434,  1529,   273,   253, 16952,   434, 10439,    84,   326,\n",
       "            309,  3698, 10793,   352,  3249,   327,   308,  5883,   390,   401,\n",
       "           7722,    13,   984,  3738,   352,   778,   320, 33657, 24842,    13,\n",
       "            352,   310,  6685, 25945,   285,   973, 14001,    13,  5043,   352,\n",
       "            434,  1694, 44198,  7493,    13,   619,  7583, 12353,  1273,   760],\n",
       "         [ 1147,  3133,   326, 34999,   556,   644,  2529,   347,   440,    14,\n",
       "           6549,  2531,  1051,   533,   326,   434,   752,  5579,   310,    15,\n",
       "           5579,   310,  5486,   281,  2085,   271,  8773,   432, 15363,  1495,\n",
       "            285,   309,  1928, 34999,  1057,   247,  1270,  2628,   275,   326,\n",
       "           2743,    15,  5733,   281,   436,   253,  3468,  1232,   273, 19678],\n",
       "         [  688,   436,  6440,  3253,  1896,   369,  3430,   285,   309,  1053,\n",
       "            626,   871,  2139,   309, 28860,  7487,   352,  1919,   253,   990,\n",
       "             15,   733,   651,   452,   644,   625,   794,  7487,  6848,  6079,\n",
       "             15,  1198, 17800,   562, 11216,   309,  1014, 10490,   399,    14,\n",
       "             53,  1004,   285,   352,   369,  1199,  1805,   685,   436,    15],\n",
       "         [ 1147,   434,  1892,   281,  2868,   326,   342,   247,  5248,   347,\n",
       "           2266,   347,   436,   581,   556,    13,   326,   436,  6440,   476,\n",
       "            320,   824,   247,   277,   438,    15,   733,   434,   824,   271,\n",
       "          16088, 19201,  3085,    15,  1359,   369,   352,  2455,  1160,    32,\n",
       "           1359,   858,   594,  1142,  1175, 14142,  5448,   598,   275,   824],\n",
       "         [    3, 30138,   368,  1364,  3523,   253,  4679,   309,   452,  3053,\n",
       "              2,  1476, 21994,  3076, 12445,   753,   436,  1512, 26350,  4759,\n",
       "           1708,   264,   436,  2199,  4672,   309,   369,  2761, 12832, 22944,\n",
       "            281,   436,  6440,   407,   247,  3331,   273,  7477,   323,   247,\n",
       "          21811,  4645,   672,   359,   497,   275,  1029,    14, 19221,    15],\n",
       "         [   42,  3534,   436,  6440,   824,   247,  1029,  1616,   984,   352,\n",
       "            369,  1663, 20295,    13,  1663, 11755,    13,   512,  1223,  1146,\n",
       "            440,  4025,   290,   784,    15,   309,  2427,   281,   923,   436,\n",
       "           3085,   672,   352,   369,  4882,   275,   253,  6778,   314,  2170,\n",
       "             13,   285,   352,   369,   253,  4055, 14705,   273,   247,  1270],\n",
       "         [   42,  5476,   309,   760,   452,  4266,   281, 13387,   323,   253,\n",
       "          41490,   557,   408,  2035,   326,   310,   346,  7580, 40451,   273,\n",
       "            247,  8237,  3446,  1422,  2168,   871,   417,   281,  1902,   247,\n",
       "          15573, 24674, 45497,   672,   368,   923,   247,  7166,  2576,   285,\n",
       "          35731, 15127,  4060,   751,   436,   285,   253,   806, 13214,   760],\n",
       "         [  510,   807,  6157,   369,   253,   807,   273,   253,  1794,  6361,\n",
       "            342,   642,  1679,   685,  1740,  7968, 46710,  1524,  3394,    13,\n",
       "           1524,   952,    13,   342, 11962,  7759,   273,  4619, 19916,    15,\n",
       "           4683,   253,  1740,  7968,   281,  1056,   352,   281,   253,  5492,\n",
       "            281,   253,   473,  1026,  1032,   275,  2393,  5826,   313,    44],\n",
       "         [ 1147,  1364,   320,   247,  1048,   673,  3622,   326,   309,   452,\n",
       "           2326,   824,   247,  3076,  6440,    15,   309,   452,   281,  1333,\n",
       "            352,   310,  1663,  1892,   281,  1056,   247,  1175,   285,    16,\n",
       "            263, 15958,  6440,   670,  2329, 15394,   533,   436,  6440,   369,\n",
       "            824,   247,  8138,   273,   673,   285,  2583,    15,  5220,   309],\n",
       "         [ 1552,  6440,   369, 12532,    27,   619,  7583, 12353,   275,   247,\n",
       "           9493, 14562,  1309,   253, 32237,  2137,    15,   733,   574, 12959,\n",
       "            432,   346,    37,  1972,  2726,   411, 14503,     3,   323,   253,\n",
       "           1943,  9791, 26286,    13, 16869,    13,  4668, 24662,   708,   432,\n",
       "            346,  7912,   610, 13619,  9903,     3,   323,   253,  4782,   708],\n",
       "         [ 7992, 42807,  4105,  2250,  3085, 28193,   253,  2455, 14872,   399,\n",
       "          15967, 38899, 40920,   285, 29040,  8652, 18195,   275,   247,  1930,\n",
       "          44568,  2554,   326,  1620,  3133,   281,  9480,    15,   380,  2250,\n",
       "            310,   387,  1682,  3388,    13,   247,  2372,   273, 34408,   414,\n",
       "            448, 20044,   275,   285,  4754,   308,   571,  2639, 30059,  1057],\n",
       "         [   42, 49294,  1869,   436,   369,   253,  5571,   434,  1445,  3085,\n",
       "            670,   253,  3722,   326, 40724,   952,    13,   534, 17132,  4722,\n",
       "             15,   733,  3548,   626,    15,  9535,   272,    13,   309,  1599,\n",
       "             13,  1339,  3815,   670,   247,   637,    14,    70,   839,  3722,\n",
       "          17778,  1288, 19884,  1288,  4725,    42,  5467, 27659, 17419,  1691],\n",
       "         [   42,  2389,   436,  6440,     2,   733,   556,  3253,     2, 11228,\n",
       "          10633, 25199,   858,   247, 15143,  2628,   820,    14, 17695,    13,\n",
       "          25091,   285,   820,    14,  7873,   804,   275,   436,  3085,    15,\n",
       "           5119,   399,   976,   729,  5134,   310,   816,  8342,  3511,    15,\n",
       "            285, 12498,   466, 32911,   310,   347, 20295,   347,  2455,    15],\n",
       "         [   42,   717,   247,  7863,  7989,   273,   253,  1984,   285,  2207,\n",
       "           4714,   310,  5604,   619, 18069,  8406,  2455,  1580, 12392, 18630,\n",
       "          15162,   387, 15679,  2354,    15,   309,  8686,   253, 17509,   562,\n",
       "            273, 23658, 24536,    13, 33718,   310,   271, 12353,   309,  2186,\n",
       "            275,  1029,  2743,   594,   672,   309,  3735,   326,   344,  4546],\n",
       "         [ 5683,  1919,   436,   747,  2952,   309,   452,   644,   247,  1943,\n",
       "            686, 25392, 19548,  1452,     8,  7989,    15,  1723,    13,   253,\n",
       "            747,  2952,   574,  8839, 39572,  1042,  1703,   352, 17778,  1288,\n",
       "          19884,  1288,  4725,   510,   747,  6416, 15951,   274,   556, 11069,\n",
       "            253,  2862,  6860,   273,   253,   921,    15,   733,   556,  1900]],\n",
       "        device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1]], device='cuda:0'), 'labels': tensor([0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        device='cuda:0')})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "dataset = load_dataset('imdb')\n",
    "dataset = concatenate_datasets([dataset[i] for i in ['train', 'test']])\n",
    "\n",
    "\n",
    "def f(data):\n",
    "    text = [i['text'] for i in data]\n",
    "    label = [i['label'] for i in data]\n",
    "\n",
    "    data = tokenizer(text,\n",
    "                     padding=True,\n",
    "                     truncation=True,\n",
    "                     max_length=50,\n",
    "                     return_tensors='pt').to(device)\n",
    "\n",
    "    data['labels'] = torch.FloatTensor(label).to(device)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "loader = torch.utils.data.DataLoader(dataset,\n",
    "                                     batch_size=16,\n",
    "                                     shuffle=True,\n",
    "                                     drop_last=True,\n",
    "                                     collate_fn=f)\n",
    "\n",
    "len(loader), next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "628012d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPTNeoXForSequenceClassification were not initialized from the model checkpoint at EleutherAI/pythia-160m and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTNeoXConfig {\n",
       "  \"_name_or_path\": \"EleutherAI/pythia-160m\",\n",
       "  \"architectures\": [\n",
       "    \"GPTNeoXForCausalLM\"\n",
       "  ],\n",
       "  \"attention_bias\": true,\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"classifier_dropout\": 0.1,\n",
       "  \"eos_token_id\": 0,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout\": 0.0,\n",
       "  \"hidden_size\": 768,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"LABEL_0\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"label2id\": {\n",
       "    \"LABEL_0\": 0\n",
       "  },\n",
       "  \"layer_norm_eps\": 1e-05,\n",
       "  \"max_position_embeddings\": 2048,\n",
       "  \"model_type\": \"gpt_neox\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 50277,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rotary_emb_base\": 10000,\n",
       "  \"rotary_pct\": 0.25,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"float16\",\n",
       "  \"transformers_version\": \"4.43.3\",\n",
       "  \"use_cache\": true,\n",
       "  \"use_parallel_residual\": true,\n",
       "  \"vocab_size\": 50304\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model_critic = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'EleutherAI/pythia-160m', num_labels=1).to(device)\n",
    "model_critic.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "model_critic.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77d000bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 3125 21.605430603027344 0.4375\n",
      "0 1000 3125 0.18605837225914001 0.6875\n",
      "0 2000 3125 0.1550552248954773 0.75\n",
      "0 3000 3125 0.17063765227794647 0.6875\n",
      "1 0 3125 0.049937088042497635 0.9375\n",
      "1 1000 3125 0.08018088340759277 0.875\n",
      "1 2000 3125 0.05100151151418686 0.9375\n",
      "1 3000 3125 0.07913466542959213 0.875\n",
      "2 0 3125 0.08428739011287689 0.9375\n",
      "2 1000 3125 0.10260467976331711 0.875\n",
      "2 2000 3125 0.043395109474658966 1.0\n",
      "2 3000 3125 0.11563587933778763 0.875\n",
      "3 0 3125 0.022636985406279564 1.0\n",
      "3 1000 3125 0.03500146418809891 0.9375\n",
      "3 2000 3125 0.07991153001785278 0.875\n",
      "3 3000 3125 0.002385583706200123 1.0\n",
      "4 0 3125 0.03368058800697327 0.9375\n",
      "4 1000 3125 0.019572339951992035 1.0\n",
      "4 2000 3125 0.021752173081040382 1.0\n",
      "4 3000 3125 0.1174311414361 0.8125\n",
      "5 0 3125 0.0034106860402971506 1.0\n",
      "5 1000 3125 0.024997200816869736 1.0\n",
      "5 2000 3125 0.07363016158342361 0.9375\n",
      "5 3000 3125 0.011998754926025867 1.0\n",
      "6 0 3125 0.008513586595654488 1.0\n",
      "6 1000 3125 0.008864747360348701 1.0\n",
      "6 2000 3125 0.008031263016164303 1.0\n",
      "6 3000 3125 0.0013737092958763242 1.0\n",
      "7 0 3125 0.0025675520300865173 1.0\n",
      "7 1000 3125 0.012287890538573265 1.0\n",
      "7 2000 3125 0.041007596999406815 0.9375\n",
      "7 3000 3125 0.041442323476076126 0.9375\n",
      "8 0 3125 0.017471428960561752 1.0\n",
      "8 1000 3125 0.0019860267639160156 1.0\n",
      "8 2000 3125 0.001201553619466722 1.0\n",
      "8 3000 3125 0.01253584586083889 1.0\n",
      "9 0 3125 0.0014720200560986996 1.0\n",
      "9 1000 3125 0.0016772388480603695 1.0\n",
      "9 2000 3125 0.013263806700706482 1.0\n",
      "9 3000 3125 0.004630787298083305 1.0\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model_critic.parameters(), lr=1e-5)\n",
    "\n",
    "for epoch in range(10):\n",
    "    for i, data in enumerate(loader):\n",
    "        out = model_critic(**data)\n",
    "        out.loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            logits = (out.logits > 0.5).squeeze(1).long()\n",
    "            acc = (logits == data['labels'].long()).sum() / len(data['labels'])\n",
    "            print(epoch, i, len(loader), out.loss.item(), acc.item())\n",
    "\n",
    "model_critic.save_pretrained('model/critic')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cuda117]",
   "language": "python",
   "name": "conda-env-cuda117-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
