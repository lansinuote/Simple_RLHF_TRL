{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21004b13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoXTokenizerFast(name_or_path='EleutherAI/pythia-160m', vocab_size=50254, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '[PAD]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<|padding|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50254: AddedToken(\"                        \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50255: AddedToken(\"                       \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50256: AddedToken(\"                      \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50257: AddedToken(\"                     \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50258: AddedToken(\"                    \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50259: AddedToken(\"                   \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50260: AddedToken(\"                  \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50261: AddedToken(\"                 \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50262: AddedToken(\"                \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50263: AddedToken(\"               \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50264: AddedToken(\"              \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50265: AddedToken(\"             \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50266: AddedToken(\"            \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50267: AddedToken(\"           \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50268: AddedToken(\"          \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50269: AddedToken(\"         \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50270: AddedToken(\"        \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50271: AddedToken(\"       \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50272: AddedToken(\"      \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50273: AddedToken(\"     \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50274: AddedToken(\"    \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50275: AddedToken(\"   \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50276: AddedToken(\"  \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50277: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "b = 8\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('EleutherAI/pythia-160m')\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91bdb9a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12500,\n",
       " tensor([[28274,   685, 18580,    15,  2752],\n",
       "         [27132,  1077, 31255,   281,   923],\n",
       "         [ 1552,   310,   247,  1175,  2217],\n",
       "         [37161,   253,  5962,  2181,   670],\n",
       "         [    3, 37222,  3995,     3,   310],\n",
       "         [48462,   581,   273,   253,  9065],\n",
       "         [   42, 11697, 10490,   346,   510],\n",
       "         [ 1051,   533,   326,   434,   271]], device='cuda:0'))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "dataset = load_dataset('imdb')\n",
    "dataset = concatenate_datasets(list((dataset.values())))\n",
    "\n",
    "\n",
    "def f(data):\n",
    "    data = [i['text'] for i in data]\n",
    "    return tokenizer(data,\n",
    "                     padding=True,\n",
    "                     truncation=True,\n",
    "                     max_length=5,\n",
    "                     return_tensors='pt').input_ids.to(device)\n",
    "\n",
    "\n",
    "loader = torch.utils.data.DataLoader(dataset,\n",
    "                                     batch_size=b,\n",
    "                                     shuffle=True,\n",
    "                                     drop_last=True,\n",
    "                                     collate_fn=f)\n",
    "\n",
    "len(loader), next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65a9e2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoModelForSequenceClassification\n",
    "from trl.trainer.utils import disable_dropout_in_model\n",
    "\n",
    "model_actor = AutoModelForCausalLM.from_pretrained('model/actor').to(device)\n",
    "model_actor_ref = AutoModelForCausalLM.from_pretrained('model/actor').to(\n",
    "    device)\n",
    "\n",
    "model_critic = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'model/critic', num_labels=1).to(device)\n",
    "model_critic_ref = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'model/critic', num_labels=1).to(device)\n",
    "\n",
    "model_actor.generation_config.eos_token_id = None\n",
    "model_actor.generation_config.pad_token_id = None\n",
    "\n",
    "for i in [model_actor, model_actor_ref, model_critic, model_critic_ref]:\n",
    "    disable_dropout_in_model(i)\n",
    "\n",
    "optimizer = torch.optim.AdamW(list(model_actor.parameters()) +\n",
    "                              list(model_critic.parameters()),\n",
    "                              lr=5e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddef38a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 15])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_value(critic, question, answer, shift=True):\n",
    "    input_ids = torch.cat((question, answer), 1)\n",
    "    attention_mask = input_ids != tokenizer.pad_token_id\n",
    "    position_ids = attention_mask.cumsum(1) - attention_mask.long()\n",
    "    input_ids = torch.masked_fill(input_ids, ~attention_mask, 0)\n",
    "\n",
    "    #[b, lens, 768]\n",
    "    last_hidden_state = critic.gpt_neox(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        position_ids=position_ids).last_hidden_state\n",
    "\n",
    "    #[b, lens]\n",
    "    value = critic.score(last_hidden_state)\n",
    "\n",
    "    if shift:\n",
    "        value = value[:, question.shape[1] - 1:-1].squeeze(-1)\n",
    "\n",
    "    return value\n",
    "\n",
    "\n",
    "get_value(model_critic,\n",
    "          torch.randint(100, 10000, [2, 5]).to(device),\n",
    "          torch.randint(100, 10000, [2, 15]).to(device)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "daab666f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 15])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_logprob(actor, question, answer):\n",
    "    input_ids = torch.cat((question, answer), 1)\n",
    "    attention_mask = input_ids != tokenizer.pad_token_id\n",
    "    position_ids = attention_mask.cumsum(1) - attention_mask.long()\n",
    "    input_ids = torch.masked_fill(input_ids, ~attention_mask, 0)\n",
    "\n",
    "    logits = actor(input_ids=input_ids,\n",
    "                   attention_mask=attention_mask,\n",
    "                   position_ids=position_ids).logits\n",
    "\n",
    "    logits = logits[:, question.shape[1] - 1:-1]\n",
    "    logits /= 0.7\n",
    "\n",
    "    logprob = logits.log_softmax(dim=-1)\n",
    "    logprob = logprob.gather(2, answer.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "    return logprob\n",
    "\n",
    "\n",
    "get_logprob(model_actor,\n",
    "            torch.randint(100, 10000, [2, 5]).to(device),\n",
    "            torch.randint(100, 10000, [2, 15]).to(device)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f41b6be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 25])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_advantage(value, reward_kl):\n",
    "    advantage = []\n",
    "    last = 0\n",
    "    for i in reversed(range(value.shape[1])):\n",
    "        value_next = 0.0\n",
    "        if i < value.shape[1] - 1:\n",
    "            value_next = value[:, i + 1]\n",
    "\n",
    "        delta = reward_kl[:, i] + value_next - value[:, i]\n",
    "\n",
    "        last = delta + 0.95 * last\n",
    "\n",
    "        advantage.append(last)\n",
    "\n",
    "    return torch.stack(advantage[::-1], axis=1)\n",
    "\n",
    "\n",
    "get_advantage(torch.randn(4, 25), torch.randn(4, 25)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ff581cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[10195,  3085,  7458,  1408, 22233],\n",
       "         [31568,  7615,   342, 15535,  6590],\n",
       "         [ 4041,   273,   253, 13918,    84],\n",
       "         [   34,    11,    49,    11,    38],\n",
       "         [ 2214,   479,   436,  6440,   369],\n",
       "         [   42,  5730,   627,   497,   625],\n",
       "         [30235,   334,   285, 16905,  1319],\n",
       "         [ 1552,   369,   247,  1270, 13402]], device='cuda:0'),\n",
       " tensor([[  323,   253,  3369,   672, 25394,   342,   247, 45591,   921,   326,\n",
       "          11210,   626,  2568,   644, 19301,   745,   313,  3022,   436,   921,\n",
       "            390,   326,   921,   534,   574,   594,  1142, 22167,   689,   253,\n",
       "           1107,   481,    29,  1288, 19884,  1288,  4725,  3378,   326,   753,\n",
       "             13,   436,   921,  5605,   441],\n",
       "         [  871,   326,   627,   310,  1900,   690,  3085,   326,  4269,  1524,\n",
       "          31795,   406,   275,   253,  1039,   326,   359,   403,   512,   594,\n",
       "           7270,   598,   275,   776,  4440,  7097,   347,   281,   871,   326,\n",
       "            627,  1364,   452,   644,   247, 24807,  2098,   275,  2564,   281,\n",
       "           1056,   436,  3085,    15, 29804],\n",
       "         [  378,  5464, 11328, 11321,  2722, 11328,   387,   521,   305,  1662,\n",
       "            386,  9041,  1682,    13,   342, 34095,  3884,   285,   295,  6785,\n",
       "           1981,  4318,    14,  1189,    15,  2732, 11328,   434,  8240,  1219,\n",
       "           1028,   472,   313, 49812,    10,   369,  1160,   715,   247, 14759,\n",
       "          28363,    13,   285,   326,   310],\n",
       "         [  310, 33917,   281,   247,    11,    38,    11,   285,   310,  5803,\n",
       "            271,   329,  4556,   380,  7484,   310, 13714,   418,   447,    15,\n",
       "           7850,   760,  1802, 17254,   629,   310,   326,   352,  8687,   247,\n",
       "            475, 24481,    11,   475,    41,  2415,  2354,    11,   327,  3601,\n",
       "          17778,  1288, 19884,  1288,  4725],\n",
       "         [ 7126,    15,  1916,   320,  1014,  4709,  1417,   597,  1014,  5176,\n",
       "            253, 34408,   414,    13,   533,  5803,    41,  2637,   275,   436,\n",
       "           6440,   369,   762,   374,    15,    22,    89,   347, 19201,   347,\n",
       "            597,  1160,   352,   562,   281,   320,    15,   380,   760,  1921,\n",
       "            309,  1918,   352,  2057,    13],\n",
       "         [ 6590,   751,   436,    13,   751,   346, 18812,   275,  5219,     3,\n",
       "            390,  2074,    15,  2596,   273,   253,  6590,   326,   690,  1908,\n",
       "            346,   783,  5075,   383,   995,   346,   783,  8334,   383,   995,\n",
       "          18109, 17325,   281,  4735, 46454, 12819,   310,   436,  3085,    15,\n",
       "          15657, 25413,  7120,   388,  6227],\n",
       "         [  310,   247,  1892,  6440,   281,  6266,    15,   733,   574,   352,\n",
       "            512,   428, 21518, 25907,    13, 26183,   689,  1299,   265,    13,\n",
       "          26183,   689,  1299,   265,    13, 26183,   689,  1299,   265,   428,\n",
       "            285,  2722,   247,  3282,   273, 22820,   275,   253,  4278,   342,\n",
       "           1077, 32126, 16226,    15,  2596],\n",
       "         [ 9037,   432,   253,  1270,   921,   686, 21276, 22757,     8,   313,\n",
       "             39,  1250,  1623,  3401,   285,   309,  1918,   352,  1264,   273,\n",
       "           3578,  6114,    15,   733,   369, 32325,  2378,   390,  7019,   247,\n",
       "           2129,    13,   533,   323,   954,   273,   253,  3515,   673,   352,\n",
       "            369,  1663, 19456,   285,   309]], device='cuda:0'),\n",
       " [45, 45, 45, 45, 45, 45, 45, 45],\n",
       " tensor([[-8.3097e-03, -1.4949e-02, -8.9600e-03, -5.6958e-03, -1.1187e-01,\n",
       "          -1.0252e-05, -2.6827e-01, -8.3374e+00, -1.8159e-02, -1.6190e-01,\n",
       "          -3.2638e+00, -8.3446e-07, -1.2876e+00, -3.2888e-01, -1.3147e+00,\n",
       "          -5.0147e-03, -2.3748e+00, -2.4429e-01, -9.7056e-02, -1.2053e-01,\n",
       "          -9.4922e-01, -2.6445e+00, -4.6585e-02, -2.8373e+00, -5.0673e+00,\n",
       "          -3.4499e+00, -2.5317e-02, -1.1344e+01, -3.6959e-01, -1.2421e+00,\n",
       "          -2.1177e-02, -1.1465e-01, -2.4536e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00, -4.0816e+00, -4.9606e-01, -1.9169e-01,\n",
       "          -2.1367e-03, -1.8035e-01, -3.2696e+00, -9.9994e+00, -1.9161e+00],\n",
       "         [-2.4269e+00, -9.6801e-03, -3.4100e+00, -3.5619e-01, -5.1543e-01,\n",
       "          -1.5218e+00, -6.6770e+00, -6.8013e-03, -1.1253e+01, -1.5567e+00,\n",
       "          -2.2530e+00, -3.3409e-04, -1.5062e-02, -3.5077e-04, -2.4032e+00,\n",
       "          -6.9826e-02, -7.3453e-01, -8.6260e-02, -7.3046e-02, -9.6641e-01,\n",
       "          -7.8005e-02, -1.1921e-07, -3.6209e-04, -3.1372e+00, -3.1369e+00,\n",
       "          -1.8954e-05, -9.4630e-01, -1.5848e+00, -3.4377e+00, -5.1997e-02,\n",
       "          -1.6015e+00, -3.3111e+00, -9.7718e-02, -1.8954e-05, -2.9411e+00,\n",
       "          -2.8064e+00, -1.3636e-01, -1.8898e-01, -1.5977e-02, -2.1842e+00,\n",
       "          -4.4259e+00, -2.2278e+00, -1.0396e-01, -3.0134e-01, -1.2495e+01],\n",
       "         [-5.5519e+00, -8.7242e-01, -1.7126e-03, -1.5323e+00, -4.9680e+00,\n",
       "          -1.0794e+00, -2.0516e-01, -1.2204e-02, -4.2570e+00, -3.7502e+00,\n",
       "          -6.3415e-02,  0.0000e+00, -4.8369e-01, -1.8880e+00, -1.8087e+00,\n",
       "          -7.6387e+00, -3.8749e+00, -2.2467e+00, -8.9625e+00, -1.1801e-03,\n",
       "          -2.4371e-02, -9.2665e+00, -1.5984e-02, -8.0532e-01, -1.0518e+00,\n",
       "          -7.4941e+00, -1.2684e+00, -1.2128e-01, -1.3247e+01, -1.7042e+00,\n",
       "          -1.1921e-07, -2.7418e-06, -1.2402e-01, -2.4041e+00, -1.4322e-01,\n",
       "          -3.1177e+00, -2.6181e+00, -8.5373e-03, -2.7281e-03, -6.4993e+00,\n",
       "          -2.9250e+00, -3.4681e-03, -2.1307e+00, -7.4911e+00, -1.4912e-01],\n",
       "         [-2.1939e-02, -1.3372e+01, -2.3842e-06, -3.4025e-02, -2.0049e+00,\n",
       "          -4.0265e-01, -4.1485e-02, -1.1814e+00, -1.3624e+00, -7.6542e+00,\n",
       "          -1.8295e+00, -8.8385e-02, -2.9637e+00, -6.7262e-01, -8.9034e-01,\n",
       "          -9.9222e-01, -7.1049e+00, -7.2170e-01, -2.0885e-01, -4.2767e-01,\n",
       "          -5.6582e+00, -2.7245e+00, -2.2401e+00, -5.3644e-06, -3.5106e+00,\n",
       "          -3.2599e-02, -2.7157e+00, -2.7098e+00, -6.9448e+00, -1.8426e+00,\n",
       "          -2.7897e+00, -4.4846e+00, -1.0923e+00, -4.0602e-01, -8.2530e-02,\n",
       "          -8.8605e-01, -3.1211e+00, -2.7865e-01, -5.0237e-01, -2.1423e-01,\n",
       "          -3.9518e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "         [-4.9086e+00, -9.1080e-02, -7.3168e+00, -2.0597e+00, -1.0529e+01,\n",
       "          -3.3559e+00, -4.4294e+00, -1.1981e+01, -1.7713e+00, -4.6774e-01,\n",
       "          -5.0216e-01, -8.3188e+00,  0.0000e+00, -3.8667e+00, -5.6383e-01,\n",
       "          -1.3621e+01, -3.1562e-04,  0.0000e+00, -1.3922e-01, -2.5529e-03,\n",
       "          -7.5867e-03, -1.0049e-01, -6.6006e+00, -2.8190e+00, -7.5926e-03,\n",
       "          -1.1119e+00, -1.9212e-02, -1.4357e-02, -8.5730e-01, -1.2077e-02,\n",
       "          -1.8180e-01, -2.9073e+00, -1.1195e-03, -2.6624e-02,  0.0000e+00,\n",
       "           0.0000e+00, -8.1389e-02, -1.5042e+00, -1.1954e+00, -3.3079e-02,\n",
       "          -1.7590e-01, -2.6997e+00, -1.8516e+00, -7.5703e+00, -8.0346e-03],\n",
       "         [-7.4274e-01, -7.1366e-02, -5.4064e-03, -3.1031e+00, -5.2621e-01,\n",
       "          -1.4870e+00, -8.1142e+00, -3.3337e-04, -5.8278e-01, -4.8238e-01,\n",
       "          -2.6391e-01, -3.8072e+00, -6.7973e-02, -4.3776e+00, -3.7750e-01,\n",
       "          -6.1579e-02, -1.8501e+00, -9.4925e-01, -9.9981e+00, -3.2655e+00,\n",
       "          -1.9639e+00, -1.4198e+00, -3.1844e+00, -2.3842e-07, -3.5602e+00,\n",
       "          -1.5610e-01, -6.9800e-01, -2.8103e+00, -1.4018e-04, -6.8450e-04,\n",
       "          -6.7525e+00, -7.7195e-02, -1.6808e+00, -8.3382e-01, -7.7314e-03,\n",
       "          -1.9141e-01, -2.8468e+00, -1.9643e+00, -4.9958e-01, -1.4780e-01,\n",
       "          -1.0999e+01, -4.5618e+00, -1.7651e+00, -6.0987e+00, -9.7964e-01],\n",
       "         [-1.0377e+00, -2.3377e-01, -5.7261e+00, -1.1667e+00, -1.3546e-03,\n",
       "          -2.4907e+00, -2.1650e-01, -4.3051e-01, -7.7793e+00, -4.4387e+00,\n",
       "          -1.3258e-01, -2.8370e+00, -8.6388e+00, -3.6871e+00, -4.6393e-02,\n",
       "          -5.3859e-01, -1.0848e+00, -1.2457e-04,  0.0000e+00, -5.7539e-03,\n",
       "          -1.3421e+00, -2.9099e-01, -1.1921e-06,  0.0000e+00, -2.4768e-02,\n",
       "          -9.4962e-02, -1.1246e-02,  0.0000e+00, -2.3842e-07, -5.5206e+00,\n",
       "          -1.4241e+00, -9.2351e+00, -1.3775e+00, -5.2720e+00, -1.7641e-04,\n",
       "          -1.2107e+00, -5.4958e-01, -2.9519e-01, -6.4603e+00, -4.0688e+00,\n",
       "          -4.2069e+00, -1.6098e+01, -7.6456e-01, -3.8093e-01, -5.6473e+00],\n",
       "         [-6.1652e+00, -1.6197e+00, -4.2442e-02, -4.2335e+00, -3.9611e-02,\n",
       "          -4.5483e+00, -3.5592e+00, -9.4838e-01, -3.0472e+00, -1.1128e+00,\n",
       "          -7.0964e+00, -2.6949e+00, -4.6663e-01, -4.3450e+00, -4.4117e-01,\n",
       "          -1.1739e+00, -1.0714e+01, -3.1899e-02, -1.2169e+00, -2.7757e+00,\n",
       "          -1.9658e+00, -5.2740e-03, -2.6240e-01, -9.9345e-01, -1.8111e+00,\n",
       "          -4.1784e+00, -9.7751e+00, -3.8858e+00, -2.3842e-07, -1.8983e-02,\n",
       "          -7.7680e-01, -2.4547e+00, -2.4504e+00, -2.8717e+00, -8.5897e-01,\n",
       "          -1.8722e-02, -2.4585e-01, -7.5601e-01, -9.2983e-06, -1.2248e-01,\n",
       "          -8.1783e-02, -5.4394e+00, -6.3618e+00, -2.0646e-01, -7.6589e-01]],\n",
       "        device='cuda:0'),\n",
       " tensor([[ 8.2887e-01,  1.0008e+00,  1.0251e+00,  1.0019e+00,  1.0049e+00,\n",
       "           1.0259e+00,  1.0416e+00,  1.0343e+00,  1.0263e+00,  1.0391e+00,\n",
       "           9.9015e-01,  9.9347e-01,  1.0066e+00,  9.9091e-01,  1.0110e+00,\n",
       "           1.0046e+00,  9.8148e-01,  1.0073e+00,  9.8927e-01,  1.0279e+00,\n",
       "           1.0250e+00,  1.0122e+00,  1.0152e+00,  1.0343e+00,  1.0211e+00,\n",
       "           1.0015e+00,  1.0105e+00,  1.0151e+00,  1.0083e+00,  1.0184e+00,\n",
       "           1.0247e+00,  1.0089e+00,  1.0214e+00,  1.0414e+00,  1.0233e+00,\n",
       "           1.0259e+00,  1.0198e+00,  1.0166e+00,  1.0238e+00,  1.0242e+00,\n",
       "           1.0338e+00,  1.0367e+00,  1.0546e+00,  1.0451e+00,  1.0290e+00],\n",
       "         [ 3.5042e-01,  3.2409e-01,  8.7328e-02,  1.0028e-01,  2.2171e-01,\n",
       "           2.9651e-01,  1.5105e-01,  8.6937e-01,  9.3460e-01,  4.7126e-01,\n",
       "           1.9246e-01,  1.0355e-01,  8.5632e-01,  8.6962e-01,  8.0346e-01,\n",
       "           4.3301e-01,  7.1463e-01,  9.0307e-01,  9.2785e-01,  9.7173e-01,\n",
       "           9.1702e-01,  9.4610e-01,  9.7266e-01,  9.8127e-01,  9.6390e-01,\n",
       "           9.4930e-01,  9.7224e-01,  9.6714e-01,  9.6068e-01,  9.4772e-01,\n",
       "           9.6321e-01,  9.6489e-01,  9.5797e-01,  9.5741e-01,  3.3675e-01,\n",
       "           6.6818e-01,  8.5102e-01,  9.2359e-01,  9.6247e-01,  3.9558e-01,\n",
       "           9.2908e-01,  9.2704e-01,  9.7415e-01,  9.4366e-01,  8.0221e-01],\n",
       "         [ 9.9244e-01,  1.0060e+00,  1.0135e+00,  9.9680e-01,  1.0028e+00,\n",
       "           1.0151e+00,  1.0301e+00,  1.0262e+00,  1.0162e+00,  1.0281e+00,\n",
       "           9.8432e-01,  9.9874e-01,  9.8997e-01,  1.0082e+00,  1.0093e+00,\n",
       "           1.0166e+00,  1.0058e+00,  1.0289e+00,  1.0232e+00,  1.0361e+00,\n",
       "           1.0266e+00,  1.0315e+00,  1.0341e+00,  1.0322e+00,  1.0243e+00,\n",
       "           9.2010e-01,  1.0258e+00,  1.0250e+00,  1.0159e+00,  1.0522e+00,\n",
       "           1.0361e+00,  1.0098e+00,  1.0237e+00,  1.0430e+00,  1.0049e+00,\n",
       "           1.0246e+00,  1.0249e+00,  1.0280e+00,  1.0151e+00,  9.9635e-01,\n",
       "           9.9321e-01,  1.0033e+00,  1.0132e+00,  1.0128e+00,  1.0044e+00],\n",
       "         [ 8.9185e-01,  9.6071e-01,  9.3949e-01,  6.4855e-01,  7.9919e-01,\n",
       "           9.0023e-01,  6.8818e-01,  7.7702e-01,  8.7588e-01,  9.7554e-01,\n",
       "           1.3094e-01,  3.1346e-02,  6.2086e-02,  2.6947e-01,  1.2815e-02,\n",
       "          -2.0040e-02, -5.5542e-03,  2.5878e-03,  5.7702e-02, -3.0666e-02,\n",
       "           1.9225e-01, -2.6679e-02, -2.7461e-02, -2.8728e-02, -3.5793e-02,\n",
       "          -2.1370e-02, -2.0202e-02, -3.2106e-02, -3.0687e-02, -1.5799e-02,\n",
       "          -8.6346e-03,  5.2151e-03, -2.2306e-02,  3.0782e-03,  1.3336e-02,\n",
       "          -1.6164e-02, -2.0430e-02,  1.6593e-02,  7.6761e-02, -2.5082e-03,\n",
       "          -7.1927e-03, -4.9029e-03,  6.0217e-02,  3.0398e-05,  7.5539e-02],\n",
       "         [ 8.9498e-01,  1.0052e+00,  4.0491e-01,  1.0146e+00,  1.0102e+00,\n",
       "           1.0185e+00,  1.0300e+00,  1.0345e+00,  1.0088e+00,  1.0071e+00,\n",
       "           1.0251e+00,  1.0091e+00,  1.0129e+00,  1.0156e+00,  1.0196e+00,\n",
       "           9.9986e-01,  1.0052e+00,  1.0107e+00,  1.0057e+00,  1.0125e+00,\n",
       "           1.0004e+00,  1.0024e+00,  1.0074e+00,  1.0108e+00,  1.0103e+00,\n",
       "           1.0109e+00,  1.0122e+00,  1.0067e+00,  1.0162e+00,  9.9319e-01,\n",
       "           1.0076e+00,  9.7678e-01,  9.8662e-01,  9.6464e-01,  8.3448e-01,\n",
       "           9.6887e-01,  8.6330e-01,  1.0041e+00,  8.2333e-01,  9.5707e-01,\n",
       "           4.9072e-01,  9.9312e-01,  1.0053e+00,  1.0016e+00,  8.8808e-01],\n",
       "         [ 4.4357e-03,  4.4121e-02,  1.9905e-02,  2.9781e-02,  3.4401e-02,\n",
       "           2.6022e-02,  6.9484e-02,  6.8792e-02,  7.4230e-02,  3.6347e-02,\n",
       "           2.9600e-02,  3.4329e-02,  2.9834e-02,  5.3479e-02,  1.7616e-01,\n",
       "           7.8575e-02,  1.3868e-01,  9.6351e-02,  1.4880e-01,  2.0634e-01,\n",
       "           4.2514e-02,  1.4145e-01,  1.1925e-01,  6.8871e-01,  9.0123e-01,\n",
       "           9.6630e-01,  9.8209e-01,  9.3441e-01,  9.1522e-01,  9.6109e-01,\n",
       "           8.2926e-01,  4.2578e-01,  9.0555e-01,  9.7545e-01,  9.6841e-01,\n",
       "           6.9863e-02,  5.0710e-01,  9.8290e-01,  9.1069e-01,  5.5739e-01,\n",
       "           9.8080e-01,  9.9098e-01,  1.0043e+00,  1.0056e+00,  1.0023e+00],\n",
       "         [ 1.0113e+00,  9.9867e-01,  1.0062e+00,  1.0274e+00,  1.0146e+00,\n",
       "           1.0115e+00,  9.9472e-01,  6.0495e-01,  9.7324e-01,  1.0011e+00,\n",
       "           9.7349e-01,  1.0025e+00,  1.0014e+00,  9.9485e-01,  1.0060e+00,\n",
       "           1.0054e+00,  9.8450e-01,  1.0017e+00,  1.0002e+00,  1.0022e+00,\n",
       "           1.0014e+00,  9.9110e-01,  1.0000e+00,  9.9686e-01,  9.9092e-01,\n",
       "           9.9954e-01,  9.8530e-01,  9.9363e-01,  9.9403e-01,  9.8306e-01,\n",
       "           9.9993e-01,  9.8964e-01,  9.9775e-01,  1.0031e+00,  9.9229e-01,\n",
       "           9.9310e-01,  9.8544e-01,  1.0075e+00,  1.0076e+00,  9.9777e-01,\n",
       "           1.0132e+00,  1.0009e+00,  9.8730e-01,  9.7785e-01,  1.0097e+00],\n",
       "         [ 1.0579e+00,  1.0339e+00,  1.0325e+00,  1.0294e+00,  1.0301e+00,\n",
       "           1.0345e+00,  1.0280e+00,  1.0217e+00,  1.0277e+00,  1.0287e+00,\n",
       "           1.0236e+00,  1.0161e+00,  1.0114e+00,  9.9346e-01,  1.0167e+00,\n",
       "           1.0065e+00,  1.0046e+00,  1.0378e+00,  1.0314e+00,  1.0402e+00,\n",
       "           1.0210e+00,  1.0327e+00,  1.0103e+00,  8.0405e-01,  1.0136e+00,\n",
       "           9.9397e-01,  1.0118e+00,  1.0162e+00,  1.0018e+00,  1.0104e+00,\n",
       "           1.0089e+00,  1.0141e+00,  9.9579e-01,  9.9309e-01,  1.0101e+00,\n",
       "           9.9091e-01,  9.9446e-01,  1.0031e+00,  1.0019e+00,  9.9800e-01,\n",
       "           9.9026e-01,  9.9572e-01,  9.9743e-01,  1.0230e+00,  1.0027e+00]],\n",
       "        device='cuda:0'),\n",
       " tensor([[ 7.4327e-01, -6.8725e-02, -1.8612e-01, -7.2141e-02, -8.2855e-02,\n",
       "          -1.8407e-01, -2.6436e-01, -2.3351e-01, -1.9820e-01, -2.6462e-01,\n",
       "          -2.5818e-02, -3.5810e-02, -9.5341e-02, -1.3969e-02, -1.0695e-01,\n",
       "          -7.2896e-02,  4.6850e-02, -7.1593e-02,  2.2529e-02, -1.6159e-01,\n",
       "          -1.4744e-01, -8.3433e-02, -9.4763e-02, -1.8689e-01, -1.2302e-01,\n",
       "          -2.3790e-02, -6.1701e-02, -8.0104e-02, -4.2256e-02, -8.7068e-02,\n",
       "          -1.1537e-01, -3.4429e-02, -9.0637e-02, -1.8766e-01, -9.8735e-02,\n",
       "          -1.0910e-01, -7.6440e-02, -5.6236e-02, -8.7347e-02, -8.6281e-02,\n",
       "          -1.3052e-01, -1.4388e-01, -2.3326e-01, -1.9008e-01, -1.1175e-01],\n",
       "         [ 1.2383e+00,  1.4430e+00,  2.7102e+00,  2.7961e+00,  2.3443e+00,\n",
       "           2.1018e+00,  2.9474e+00, -4.7955e-01, -8.2284e-01,  1.4575e+00,\n",
       "           2.9356e+00,  3.5425e+00, -2.5347e-02, -8.5205e-02,  2.4894e-01,\n",
       "           2.1214e+00,  8.3356e-01, -5.6370e-02, -1.7520e-01, -3.9579e-01,\n",
       "          -1.3519e-01, -2.7971e-01, -4.1922e-01, -4.7631e-01, -4.0661e-01,\n",
       "          -3.4708e-01, -4.7203e-01, -4.6340e-01, -4.4754e-01, -3.9838e-01,\n",
       "          -4.8879e-01, -5.1493e-01, -4.9947e-01, -5.1500e-01,  2.5678e+00,\n",
       "           1.0545e+00,  2.0413e-01, -1.3984e-01, -3.3356e-01,  2.4901e+00,\n",
       "          -3.7210e-02, -2.1036e-02, -2.4959e-01, -1.0240e-01,  6.0714e-01],\n",
       "         [-5.1029e-02, -1.1332e-01, -1.4883e-01, -6.5361e-02, -9.0621e-02,\n",
       "          -1.4903e-01, -2.2411e-01, -2.0832e-01, -1.6142e-01, -2.2118e-01,\n",
       "          -6.1927e-03, -7.0635e-02, -2.2524e-02, -1.0667e-01, -1.1016e-01,\n",
       "          -1.4435e-01, -8.9935e-02, -2.0214e-01, -1.7630e-01, -2.4237e-01,\n",
       "          -1.9946e-01, -2.2627e-01, -2.4329e-01, -2.3878e-01, -2.0405e-01,\n",
       "           3.1413e-01, -1.8944e-01, -1.8765e-01, -1.4430e-01, -3.2516e-01,\n",
       "          -2.5394e-01, -1.2755e-01, -1.9600e-01, -2.9461e-01, -1.1169e-01,\n",
       "          -2.0818e-01, -2.1264e-01, -2.3162e-01, -1.7142e-01, -7.8599e-02,\n",
       "          -5.9056e-02, -1.0443e-01, -1.5145e-01, -1.4955e-01, -1.0748e-01],\n",
       "         [-2.8283e+00, -3.3133e+00, -3.3737e+00, -2.0892e+00, -2.9441e+00,\n",
       "          -3.5961e+00, -2.7175e+00, -3.2966e+00, -3.9563e+00, -4.6546e+00,\n",
       "          -6.7044e-01, -1.9999e-01, -3.5619e-01, -1.4035e+00, -1.8661e-01,\n",
       "          -2.4264e-02, -8.9974e-02, -1.2743e-01, -4.0163e-01,  2.6852e-02,\n",
       "          -1.0779e+00, -3.2460e-02, -2.2290e-02, -9.1682e-03,  3.3626e-02,\n",
       "          -2.8721e-02, -2.8101e-02,  3.7881e-02,  4.0749e-02, -2.3546e-02,\n",
       "          -5.2626e-02, -1.1665e-01,  2.2724e-02, -9.4978e-02, -1.4328e-01,\n",
       "           4.5844e-03,  3.4116e-02, -1.4116e-01, -4.4133e-01, -6.0418e-02,\n",
       "          -3.2219e-02, -3.7392e-02, -3.5686e-01, -6.6864e-02, -4.3980e-01],\n",
       "         [ 2.1823e-01, -3.1300e-01,  2.6785e+00, -2.1960e-01, -2.0139e-01,\n",
       "          -2.4552e-01, -3.0767e-01, -3.3881e-01, -2.1993e-01, -2.1510e-01,\n",
       "          -3.0843e-01, -2.3672e-01, -2.6021e-01, -2.7935e-01, -3.0603e-01,\n",
       "          -2.1562e-01, -2.4591e-01, -2.7793e-01, -2.5968e-01, -2.9936e-01,\n",
       "          -2.4681e-01, -2.6179e-01, -2.9246e-01, -3.1683e-01, -3.2325e-01,\n",
       "          -3.3532e-01, -3.5156e-01, -3.3441e-01, -3.9171e-01, -2.8933e-01,\n",
       "          -3.6858e-01, -2.2598e-01, -2.7912e-01, -1.7599e-01,  4.7323e-01,\n",
       "          -1.6551e-01,  3.6136e-01, -3.1515e-01,  5.7945e-01, -5.0474e-02,\n",
       "           2.2856e+00, -9.7076e-02, -1.5531e-01, -1.3670e-01,  4.3134e-01],\n",
       "         [ 1.3014e+00,  1.1795e+00,  1.3706e+00,  1.4013e+00,  1.4599e+00,\n",
       "           1.5866e+00,  1.4609e+00,  1.5492e+00,  1.6115e+00,  1.8936e+00,\n",
       "           2.0350e+00,  2.1264e+00,  2.2687e+00,  2.2779e+00,  1.7927e+00,\n",
       "           2.3827e+00,  2.2157e+00,  2.5518e+00,  2.4319e+00,  2.2803e+00,\n",
       "           3.2271e+00,  2.9105e+00,  3.1826e+00,  5.1192e-01, -5.1528e-01,\n",
       "          -8.5967e-01, -9.7586e-01, -7.8093e-01, -7.1816e-01, -9.7726e-01,\n",
       "          -3.6187e-01,  1.6436e+00, -6.5974e-01, -1.0358e+00, -1.0472e+00,\n",
       "           3.3964e+00,  1.3979e+00, -8.9853e-01, -5.7695e-01,  1.1664e+00,\n",
       "          -8.8041e-01, -9.6967e-01, -1.0793e+00, -1.1344e+00, -1.1700e+00],\n",
       "         [-2.7031e-01, -2.1331e-01, -2.5435e-01, -3.6584e-01, -3.1277e-01,\n",
       "          -3.0609e-01, -2.3017e-01,  1.7137e+00, -2.8812e-02, -1.6171e-01,\n",
       "          -2.4166e-02, -1.6241e-01, -1.5750e-01, -1.2512e-01, -1.7949e-01,\n",
       "          -1.7817e-01, -7.4892e-02, -1.5685e-01, -1.4972e-01, -1.5978e-01,\n",
       "          -1.5588e-01, -1.0477e-01, -1.4678e-01, -1.3085e-01, -1.0010e-01,\n",
       "          -1.4048e-01, -6.8710e-02, -1.0598e-01, -1.0561e-01, -4.8359e-02,\n",
       "          -1.2726e-01, -7.4556e-02, -1.1106e-01, -1.3568e-01, -8.0806e-02,\n",
       "          -8.1147e-02, -3.9184e-02, -1.4358e-01, -1.4349e-01, -9.4067e-02,\n",
       "          -1.6810e-01, -1.0779e-01, -3.7329e-02,  1.5928e-02, -1.3455e-01],\n",
       "         [-3.5466e-01, -2.4553e-01, -2.4357e-01, -2.3281e-01, -2.4066e-01,\n",
       "          -2.6734e-01, -2.4096e-01, -2.1423e-01, -2.4723e-01, -2.5757e-01,\n",
       "          -2.3750e-01, -2.0480e-01, -1.8373e-01, -9.5963e-02, -2.0926e-01,\n",
       "          -1.6123e-01, -1.5213e-01, -3.1827e-01, -2.9504e-01, -3.4649e-01,\n",
       "          -2.6077e-01, -3.2506e-01, -2.2250e-01,  8.0476e-01, -1.9246e-01,\n",
       "          -9.6277e-02, -1.8247e-01, -2.0597e-01, -1.3723e-01, -1.7922e-01,\n",
       "          -1.7335e-01, -2.0022e-01, -1.1142e-01, -9.5829e-02, -1.7805e-01,\n",
       "          -8.3455e-02, -9.7608e-02, -1.3773e-01, -1.3108e-01, -1.1066e-01,\n",
       "          -6.9854e-02, -9.2841e-02, -9.8339e-02, -2.2352e-01, -1.2561e-01]],\n",
       "        device='cuda:0'),\n",
       " tensor([[ 1.0173e+00,  1.0182e+00,  1.0178e+00,  1.0186e+00,  1.0194e+00,\n",
       "           1.0190e+00,  1.0178e+00,  1.0170e+00,  1.0165e+00,  1.0153e+00,\n",
       "           1.0166e+00,  1.0178e+00,  1.0184e+00,  1.0198e+00,  1.0203e+00,\n",
       "           1.0211e+00,  1.0232e+00,  1.0241e+00,  1.0259e+00,  1.0258e+00,\n",
       "           1.0258e+00,  1.0265e+00,  1.0271e+00,  1.0268e+00,  1.0271e+00,\n",
       "           1.0284e+00,  1.0294e+00,  1.0301e+00,  1.0313e+00,  1.0319e+00,\n",
       "           1.0323e+00,  1.0335e+00,  1.0342e+00,  1.0338e+00,  1.0344e+00,\n",
       "           1.0348e+00,  1.0356e+00,  1.0366e+00,  1.0373e+00,  1.0379e+00,\n",
       "           1.0382e+00,  1.0382e+00,  1.0374e+00,  1.0370e+00,  1.0374e+00],\n",
       "         [ 6.4310e-01,  6.5989e-01,  6.9003e-01,  7.2106e-01,  7.4735e-01,\n",
       "           7.7107e-01,  8.0371e-01,  8.0025e-01,  7.9318e-01,  8.1012e-01,\n",
       "           8.4263e-01,  8.8153e-01,  8.8286e-01,  8.8356e-01,  8.8777e-01,\n",
       "           9.1171e-01,  9.2208e-01,  9.2308e-01,  9.2283e-01,  9.2025e-01,\n",
       "           9.2042e-01,  9.1907e-01,  9.1625e-01,  9.1283e-01,  9.1014e-01,\n",
       "           9.0808e-01,  9.0470e-01,  9.0142e-01,  8.9830e-01,  8.9570e-01,\n",
       "           8.9214e-01,  8.8832e-01,  8.8465e-01,  8.8082e-01,  9.0946e-01,\n",
       "           9.2215e-01,  9.2590e-01,  9.2602e-01,  9.2410e-01,  9.5192e-01,\n",
       "           9.5312e-01,  9.5449e-01,  9.5346e-01,  9.5397e-01,  9.6196e-01],\n",
       "         [ 1.0136e+00,  1.0140e+00,  1.0140e+00,  1.0149e+00,  1.0156e+00,\n",
       "           1.0156e+00,  1.0148e+00,  1.0142e+00,  1.0141e+00,  1.0134e+00,\n",
       "           1.0149e+00,  1.0157e+00,  1.0171e+00,  1.0176e+00,  1.0180e+00,\n",
       "           1.0181e+00,  1.0187e+00,  1.0182e+00,  1.0179e+00,  1.0170e+00,\n",
       "           1.0165e+00,  1.0157e+00,  1.0147e+00,  1.0138e+00,  1.0132e+00,\n",
       "           1.0181e+00,  1.0177e+00,  1.0174e+00,  1.0174e+00,  1.0156e+00,\n",
       "           1.0145e+00,  1.0148e+00,  1.0143e+00,  1.0128e+00,  1.0132e+00,\n",
       "           1.0126e+00,  1.0120e+00,  1.0111e+00,  1.0109e+00,  1.0117e+00,\n",
       "           1.0126e+00,  1.0131e+00,  1.0131e+00,  1.0132e+00,  1.0136e+00],\n",
       "         [ 3.2805e-01,  2.9475e-01,  2.6081e-01,  2.4041e-01,  2.1100e-01,\n",
       "           1.7472e-01,  1.4770e-01,  1.1457e-01,  7.4506e-02,  2.7084e-02,\n",
       "           2.1617e-02,  2.1105e-02,  1.8948e-02,  5.7628e-03,  5.3916e-03,\n",
       "           6.7302e-03,  7.3767e-03,  7.6287e-03,  4.9933e-03,  6.8701e-03,\n",
       "          -2.8867e-03, -1.6345e-03, -2.7516e-04,  1.2223e-03,  3.1705e-03,\n",
       "           4.4621e-03,  5.7602e-03,  7.7531e-03,  9.7763e-03,  1.1122e-02,\n",
       "           1.2162e-02,  1.2528e-02,  1.4361e-02,  1.4955e-02,  1.5040e-02,\n",
       "           1.6683e-02,  1.8636e-02,  1.8743e-02,  1.5690e-02,  1.6648e-02,\n",
       "           1.7902e-02,  1.9103e-02,  1.6939e-02,  1.7829e-02,  1.4791e-02],\n",
       "         [ 9.7283e-01,  9.7113e-01,  1.0009e+00,  1.0002e+00,  9.9968e-01,\n",
       "           9.9869e-01,  9.9704e-01,  9.9507e-01,  9.9435e-01,  9.9368e-01,\n",
       "           9.9202e-01,  9.9112e-01,  9.8998e-01,  9.8863e-01,  9.8700e-01,\n",
       "           9.8633e-01,  9.8533e-01,  9.8400e-01,  9.8286e-01,  9.8130e-01,\n",
       "           9.8029e-01,  9.7913e-01,  9.7764e-01,  9.7590e-01,  9.7409e-01,\n",
       "           9.7215e-01,  9.7005e-01,  9.6812e-01,  9.6559e-01,  9.6414e-01,\n",
       "           9.6185e-01,  9.6106e-01,  9.5972e-01,  9.5946e-01,  9.6604e-01,\n",
       "           9.6589e-01,  9.7129e-01,  9.6956e-01,  9.7726e-01,  9.7832e-01,\n",
       "           1.0040e+00,  1.0046e+00,  1.0045e+00,  1.0047e+00,  1.0108e+00],\n",
       "         [ 3.1040e-01,  3.2442e-01,  3.4045e-01,  3.5680e-01,  3.7377e-01,\n",
       "           3.9207e-01,  4.0905e-01,  4.2695e-01,  4.4552e-01,  4.6705e-01,\n",
       "           4.9008e-01,  5.1406e-01,  5.3955e-01,  5.6513e-01,  5.8561e-01,\n",
       "           6.1229e-01,  6.3722e-01,  6.6569e-01,  6.9289e-01,  7.1850e-01,\n",
       "           7.5408e-01,  7.8632e-01,  8.2143e-01,  8.2841e-01,  8.2458e-01,\n",
       "           8.1712e-01,  8.0844e-01,  8.0181e-01,  7.9584e-01,  7.8714e-01,\n",
       "           7.8493e-01,  8.0383e-01,  7.9847e-01,  7.8916e-01,  7.7973e-01,\n",
       "           8.1709e-01,  8.3340e-01,  8.2553e-01,  8.2105e-01,  8.3493e-01,\n",
       "           8.2725e-01,  8.1864e-01,  8.0886e-01,  7.9851e-01,  7.8778e-01],\n",
       "         [ 9.8627e-01,  9.8562e-01,  9.8454e-01,  9.8228e-01,  9.8058e-01,\n",
       "           9.7895e-01,  9.7812e-01,  9.9776e-01,  9.9905e-01,  9.9894e-01,\n",
       "           1.0003e+00,  1.0002e+00,  1.0001e+00,  1.0004e+00,  1.0001e+00,\n",
       "           9.9980e-01,  1.0006e+00,  1.0005e+00,  1.0006e+00,  1.0005e+00,\n",
       "           1.0004e+00,  1.0009e+00,  1.0010e+00,  1.0012e+00,  1.0017e+00,\n",
       "           1.0018e+00,  1.0027e+00,  1.0032e+00,  1.0037e+00,  1.0048e+00,\n",
       "           1.0050e+00,  1.0058e+00,  1.0062e+00,  1.0064e+00,  1.0071e+00,\n",
       "           1.0079e+00,  1.0091e+00,  1.0092e+00,  1.0092e+00,  1.0098e+00,\n",
       "           1.0097e+00,  1.0101e+00,  1.0113e+00,  1.0131e+00,  1.0133e+00],\n",
       "         [ 1.0151e+00,  1.0141e+00,  1.0131e+00,  1.0123e+00,  1.0113e+00,\n",
       "           1.0101e+00,  1.0091e+00,  1.0085e+00,  1.0075e+00,  1.0064e+00,\n",
       "           1.0055e+00,  1.0049e+00,  1.0045e+00,  1.0051e+00,  1.0045e+00,\n",
       "           1.0044e+00,  1.0044e+00,  1.0027e+00,  1.0011e+00,  9.9909e-01,\n",
       "           9.9793e-01,  9.9610e-01,  9.9536e-01,  1.0054e+00,  1.0050e+00,\n",
       "           1.0056e+00,  1.0052e+00,  1.0047e+00,  1.0048e+00,  1.0045e+00,\n",
       "           1.0043e+00,  1.0038e+00,  1.0042e+00,  1.0048e+00,  1.0045e+00,\n",
       "           1.0052e+00,  1.0058e+00,  1.0059e+00,  1.0061e+00,  1.0066e+00,\n",
       "           1.0074e+00,  1.0080e+00,  1.0086e+00,  1.0078e+00,  1.0081e+00]],\n",
       "        device='cuda:0'))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from trl.trainer.utils import first_true_indices\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_data(question):\n",
    "    #====answer====\n",
    "    answer = model_actor.generate(\n",
    "        input_ids=question,\n",
    "        attention_mask=(question != tokenizer.pad_token_id).long(),\n",
    "        min_length=-1,\n",
    "        max_length=50,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        top_k=0.0,\n",
    "        top_p=1.0,\n",
    "        do_sample=True)\n",
    "\n",
    "    answer = answer[:, question.shape[1]:]\n",
    "\n",
    "    #求结束位置\n",
    "    ends = first_true_indices(answer == tokenizer.pad_token_id).tolist()\n",
    "\n",
    "    #====prob,value====\n",
    "    prob_old = get_logprob(model_actor, question, answer)\n",
    "    prob_ref = get_logprob(model_actor_ref, question, answer)\n",
    "    value_old = get_value(model_critic, question, answer)\n",
    "    #这里因为有可能取到最后一个字,所以不能偏移,如果偏移的话,最后一个字的值会被裁剪掉.\n",
    "    value_ref = get_value(model_critic_ref, question, answer, shift=False)\n",
    "\n",
    "    #end以后的值value归零\n",
    "    for i, end in enumerate(ends):\n",
    "        prob_old[i, end:] = 1.0\n",
    "        prob_ref[i, end:] = 1.0\n",
    "        value_old[i, end + 1:] = 0.0\n",
    "\n",
    "    #====reward====\n",
    "    reward = []\n",
    "    for i, end in enumerate(ends):\n",
    "        #没有eos符号的,置为-1\n",
    "        if tokenizer.eos_token_id not in answer[i]:\n",
    "            #reward.append(-1)\n",
    "            #continue\n",
    "            pass\n",
    "        #取最后一个字的value作为reward\n",
    "        reward.append(value_ref[i, end + question.shape[1] - 1])\n",
    "    reward = torch.FloatTensor(reward).to(device)\n",
    "\n",
    "    #====advantage====\n",
    "    #计算kl散度\n",
    "    reward_kl = -0.05 * (prob_old - prob_ref)\n",
    "\n",
    "    #把reward加在最后一个字的kl散度上\n",
    "    for i, end in enumerate(ends):\n",
    "        if end == len(answer[i]):\n",
    "            end = -1\n",
    "        #assert end == -1\n",
    "\n",
    "        reward_kl[i, end] += reward[i]\n",
    "\n",
    "    advantage = get_advantage(value_old, reward_kl)\n",
    "    returns = advantage + value_old\n",
    "\n",
    "    #标准化,保持数值稳定\n",
    "    select = torch.cat([adv[:end] for adv, end in zip(advantage, ends)])\n",
    "    advantage = (advantage - select.mean()) / (select.var() + 1e-8)**0.5\n",
    "\n",
    "    #end以后的值归零\n",
    "    for i, end in enumerate(ends):\n",
    "        advantage[i, end:] = 0\n",
    "\n",
    "    return question, answer, ends, prob_old, value_old, advantage, returns\n",
    "\n",
    "\n",
    "get_data(next(iter(loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "479d1c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(question, answer, ends, prob_old, value_old, advantage, returns):\n",
    "    for _ in range(4):\n",
    "        #重新计算value和prob\n",
    "        prob_new = get_logprob(model_actor, question, answer)\n",
    "        value_new = get_value(model_critic, question, answer)\n",
    "\n",
    "        #end以后的值value归零\n",
    "        for i, end in enumerate(ends):\n",
    "            prob_new[i, end:] = 1.0\n",
    "            value_new[i, end + 1:] = 0\n",
    "\n",
    "        #计算critic部分的loss\n",
    "        value_clip = torch.clamp(value_new, value_old - 0.2, value_old + 0.2)\n",
    "        loss_vf1 = (value_new - returns)**2\n",
    "        loss_vf2 = (value_clip - returns)**2\n",
    "        loss_vf = torch.max(loss_vf1, loss_vf2)\n",
    "\n",
    "        #计算actor部分的loss\n",
    "        ratio = (prob_new - prob_old).exp()\n",
    "        loss_pg1 = -advantage * ratio\n",
    "        loss_pg2 = -advantage * torch.clamp(ratio, 0.8, 1.2)\n",
    "        loss_pg = torch.max(loss_pg1, loss_pg2)\n",
    "\n",
    "        #丢弃end之后的部分\n",
    "        loss_vf = [xi[:end + 1] for xi, end in zip(loss_vf, ends)]\n",
    "        loss_pg = [xi[:end + 1] for xi, end in zip(loss_pg, ends)]\n",
    "        loss_vf = torch.cat(loss_vf).mean()\n",
    "        loss_pg = torch.cat(loss_pg).mean()\n",
    "\n",
    "        loss = loss_pg + 0.05 * loss_vf\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "\n",
    "train(*get_data(next(iter(loader))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d07c811",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "This is the second film\n",
      "--------\n",
      " made by the Australian Film Company (ACTC), following a similar one, Shadow Brooke. In the Shadow Brooke, Shadow Brooke dealt with the trials and tribulations of Angels. In this one, it's based\n",
      "====================\n",
      "200\n",
      "this is definately one\n",
      "--------\n",
      " of my favorite horror movies. it is what made movies so great that you would be missing out the many times when you were watching it.<br /><br />there are at least 3 genre films which I absolutely love.it\n",
      "====================\n",
      "400\n",
      "What the hell is that\n",
      "--------\n",
      "? A very good movie. The acting is great to the extreme in all the characters, the story is superb, the chemistry between the actors is excellent...the story was absolutely wonderful.<br /><br />The movie was story\n",
      "====================\n",
      "600\n",
      "I don't understand how\n",
      "--------\n",
      " this movie lasted, it's amazing. The movie was fantastic, it was fantastic. The cast was great. The kids in it were so cute, and the action was fantastic, and it was all good. The movie was\n",
      "====================\n",
      "800\n",
      "This film is a perfect\n",
      "--------\n",
      " friend to read. The story is simple, wonderful, quirky, and very heart-warming. It's not dark, but it has so much love. It's also a very good children of the universe. It\n",
      "====================\n",
      "1000\n",
      "I have to say this\n",
      "--------\n",
      " is the very best movie I have ever seen. This movie is definitely one of the best horror movies I have ever seen. I loved the first film and the sequel but I have to say that this is one of the best\n",
      "====================\n",
      "1200\n",
      "The fact that there are\n",
      "--------\n",
      " no plot twists is an unusual finding. This is a classic of the genre. Although, this movie is actually unbearably slow. I recommend this Ninja movie. A very interesting topic and in some ways a topic that\n",
      "====================\n",
      "1400\n",
      "River Queen attempts to\n",
      "--------\n",
      " be one of the most powerful films I have seen this year.<br /><br />The movie is excellent. It was well acted, well told, well acted. The director was a wonderful director. There are other movies like\n",
      "====================\n",
      "1600\n",
      "If this movie had a\n",
      "--------\n",
      " lot of talent, I would describe it as a classic.<br /><br />The plot is great.<br /><br />The acting is brilliant.<br /><br />The special effects are amazing. It's a great movie\n",
      "====================\n",
      "1800\n",
      "Take Kevin Smith's cult\n",
      "--------\n",
      " movie of a piece of cheese, this is one of the best animated movies I've seen in a long time. It is one of the most interesting films I have ever seen. The cast is excellent, and the film is\n",
      "====================\n",
      "2000\n",
      "The persona was always unique\n",
      "--------\n",
      ". <br /><br />I absolutely loved this movie. It was so hilarious. I absolutely loved the script.<br /><br />The music was great.<br /><br />The acting was good.<br /><br />\n",
      "====================\n",
      "2200\n",
      "In \"Hoot\",\n",
      "--------\n",
      " it's truly great movie. <br /><br />This movie is one of the best movies that I have ever seen. I think it's the funniest movie I have ever seen.<br /><br />The plot\n",
      "====================\n",
      "2400\n",
      "For Trekkies one\n",
      "--------\n",
      " of the best. It was one of the best anime movies I have seen.<br /><br />I really love this anime. I feel this movie is one of the best anime films I have seen.<br /><br />\n",
      "====================\n",
      "2600\n",
      "Le roi danse\n",
      "--------\n",
      " is one of my favorite movies. This film is one of the best films I have seen in a long time. <br /><br />It is totally original and all the characters are interesting. <br /><br />The\n",
      "====================\n",
      "2800\n",
      "I had hoped this movie\n",
      "--------\n",
      " was one of the best I have ever seen. This is one of the best movies I've ever seen. The story is amazing, and the acting is perfect. I first saw this film when I was a kid and it\n",
      "====================\n",
      "3000\n",
      "At first glance this documentary\n",
      "--------\n",
      " is an excellent film. I think that it is one of the best documentaries I have ever seen. It has very important characters in it. In this case, it is the three men who act in this movie. It\n",
      "====================\n",
      "3200\n",
      "Very silly high school/\n",
      "--------\n",
      "movie characters. This is one of the best shows I have ever seen. The acting by the entire cast is fantastic. The story line is fantastic. I love the state police. It had fun parts. I would recommend it\n",
      "====================\n",
      "3400\n",
      "Ronald Reagan's third\n",
      "--------\n",
      " film is one of the funniest films I ever saw.<br /><br />It is probably one of the funniest films I have seen in a long time. The plot is very witty and the acting is\n",
      "====================\n",
      "3600\n",
      "what a depressing film this\n",
      "--------\n",
      " is one of the funniest films i have ever seen. It was one of the most enjoyable films i have seen in a long time. I would recommend it to anyone. It is one of the funniest films\n",
      "====================\n",
      "3800\n",
      "I'm a huge Steve\n",
      "--------\n",
      ". This movie was absolutely hilarious. This was my favorite movie of the year. <br /><br />This movie's one of the best movies I've seen in a very long time. I think it's a marvelous\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "for i in range(4000):\n",
    "    train(*get_data(next(iter(loader))))\n",
    "\n",
    "    if i % 200 == 0:\n",
    "        print(i)\n",
    "        input_ids = next(iter(loader))[0:1]\n",
    "\n",
    "        gen = model_actor.generate(input_ids=input_ids,\n",
    "                                   min_length=-1,\n",
    "                                   max_length=50,\n",
    "                                   pad_token_id=tokenizer.pad_token_id,\n",
    "                                   eos_token_id=tokenizer.eos_token_id,\n",
    "                                   top_k=0.0,\n",
    "                                   top_p=1.0,\n",
    "                                   do_sample=True)\n",
    "\n",
    "        print(tokenizer.decode(input_ids[0]))\n",
    "        print('--------')\n",
    "        print(tokenizer.decode(gen[0, input_ids.shape[1]:]))\n",
    "        print('====================')\n",
    "\n",
    "model_actor.save_pretrained('model/ppo')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cuda117]",
   "language": "python",
   "name": "conda-env-cuda117-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
