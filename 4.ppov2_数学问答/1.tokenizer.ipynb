{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "           2,  8, 13,  4,  5,  6,  7, 17,  4,  5,  6,  7,  8,  9],\n",
       "         [ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2, 12, 12, 12,\n",
       "          12, 13, 12, 12, 12, 12, 17, 12, 12, 12, 12, 12, 12, 12]],\n",
       "        device='cuda:0'),\n",
       " 'attention_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "\n",
    "    def __init__(self):\n",
    "        vocab = ['B', 'E', 'P'] + list('0123456789+-*/=.')\n",
    "        self.vocab_encode = {j: i for i, j in enumerate(vocab)}\n",
    "        self.vocab_decode = {i: j for i, j in enumerate(vocab)}\n",
    "\n",
    "        self.bos_token = 'B'\n",
    "        self.eos_token = 'E'\n",
    "        self.pad_token = 'P'\n",
    "        self.bos_token_id = self.vocab_encode['B']\n",
    "        self.eos_token_id = self.vocab_encode['E']\n",
    "        self.pad_token_id = self.vocab_encode['P']\n",
    "        self.eq_token_id = self.vocab_encode['=']\n",
    "\n",
    "    def encode(self,\n",
    "               text,\n",
    "               padding=True,\n",
    "               truncation=True,\n",
    "               max_length=128,\n",
    "               add_bos_token=True,\n",
    "               add_eos_token=True,\n",
    "               padding_side='right',\n",
    "               device='cpu'):\n",
    "        input_ids = [[self.vocab_encode[j] for j in i] for i in text]\n",
    "\n",
    "        if add_bos_token:\n",
    "            input_ids = [[self.bos_token_id] + i for i in input_ids]\n",
    "        if add_eos_token:\n",
    "            input_ids = [i + [self.eos_token_id] for i in input_ids]\n",
    "\n",
    "        if padding:\n",
    "            lens = max([len(i) for i in input_ids])\n",
    "\n",
    "            if padding == 'max_length':\n",
    "                lens = max_length\n",
    "\n",
    "            if padding_side == 'right':\n",
    "                input_ids = [\n",
    "                    i + [self.pad_token_id] * (lens - len(i))\n",
    "                    for i in input_ids\n",
    "                ]\n",
    "\n",
    "            if padding_side == 'left':\n",
    "                input_ids = [[self.pad_token_id] * (lens - len(i)) + i\n",
    "                             for i in input_ids]\n",
    "\n",
    "        if truncation:\n",
    "            input_ids = [i[:max_length] for i in input_ids]\n",
    "\n",
    "            for i in input_ids:\n",
    "                if add_eos_token and i[-1] != self.eos_token_id and i[\n",
    "                        -1] != self.pad_token_id:\n",
    "                    i[-1] = self.eos_token_id\n",
    "\n",
    "        input_ids = torch.LongTensor(input_ids).to(device)\n",
    "        attention_mask = (input_ids != self.pad_token_id).long().to(device)\n",
    "\n",
    "        return {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
    "\n",
    "    def decode(self, input_ids, ignore_pad=True):\n",
    "        if (hasattr(input_ids, 'tolist')):\n",
    "            input_ids = input_ids.tolist()\n",
    "\n",
    "        if ignore_pad:\n",
    "            if tokenizer.bos_token_id in input_ids:\n",
    "                index = input_ids.index(tokenizer.bos_token_id)\n",
    "                input_ids = input_ids[index:]\n",
    "\n",
    "            if tokenizer.eos_token_id in input_ids:\n",
    "                index = input_ids.index(tokenizer.eos_token_id) + 1\n",
    "                input_ids = input_ids[:index]\n",
    "\n",
    "        text = [self.vocab_decode[i] for i in input_ids]\n",
    "\n",
    "        return ''.join(text)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vocab_encode)\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.encode(*args, **kwargs)\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "tokenizer(['5+1234=123456', '9999+9999=9999999'],\n",
    "          padding='max_length',\n",
    "          truncation=True,\n",
    "          max_length=32,\n",
    "          add_bos_token=False,\n",
    "          add_eos_token=False,\n",
    "          padding_side='left',\n",
    "          device='cuda')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pt2]",
   "language": "python",
   "name": "conda-env-pt2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
