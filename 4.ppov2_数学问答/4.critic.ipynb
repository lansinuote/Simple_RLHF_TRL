{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ec54a62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Tokenizer at 0x7f40f525c280>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "%run 1.tokenizer.ipynb\n",
    "\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "847c02a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125000,\n",
       " {'input_ids': tensor([[ 0,  7,  3,  5, 13,  7,  5,  7,  4, 17,  7,  9,  7,  6,  1,  2,  2,  2,\n",
       "            2,  2],\n",
       "          [ 0, 10,  3,  9,  6, 16, 11,  9,  5,  7, 17,  3,  1,  2,  2,  2,  2,  2,\n",
       "            2,  2],\n",
       "          [ 0, 11,  6,  7,  8, 13,  8,  5,  4, 10, 17,  9,  5,  4, 10,  1,  2,  2,\n",
       "            2,  2],\n",
       "          [ 0, 10,  7,  8, 12, 15,  6, 11,  9,  5, 17,  5, 11, 11,  3,  9,  9,  8,\n",
       "           11,  1],\n",
       "          [ 0,  6, 10,  9, 11, 15, 12, 11,  4, 10, 17,  6,  9, 12, 12,  3,  7,  8,\n",
       "            9,  1],\n",
       "          [ 0, 12,  4,  9,  8, 16, 11, 10,  9, 11, 17,  4,  1,  2,  2,  2,  2,  2,\n",
       "            2,  2],\n",
       "          [ 0,  9,  5,  3, 12, 16,  4,  4,  8, 12, 17,  8,  1,  2,  2,  2,  2,  2,\n",
       "            2,  2],\n",
       "          [ 0, 11,  9,  9, 10, 15,  4,  8, 12, 17,  4,  6, 10, 11,  3,  8,  6,  1,\n",
       "            2,  2],\n",
       "          [ 0, 12, 12,  7, 14,  8,  4, 12,  8, 17, 14,  7,  5,  3,  4,  1,  2,  2,\n",
       "            2,  2],\n",
       "          [ 0, 12,  4,  9,  8, 16,  5,  7,  8,  4, 17, 10, 12,  1,  2,  2,  2,  2,\n",
       "            2,  2],\n",
       "          [ 0,  7, 11, 10,  5, 15,  7, 10,  3,  3, 17,  5,  5, 11, 12, 11,  7,  3,\n",
       "            3,  1],\n",
       "          [ 0,  6,  7,  6,  7, 15,  6,  7,  7, 10, 17,  6,  4, 12, 11,  8,  9,  8,\n",
       "            1,  2],\n",
       "          [ 0,  6,  9,  5,  3, 14, 10,  6,  8, 10, 17, 14,  6, 10,  6, 10,  1,  2,\n",
       "            2,  2],\n",
       "          [ 0,  7, 10,  7,  9, 14,  9, 11,  7,  4, 17,  5,  8,  3,  1,  2,  2,  2,\n",
       "            2,  2],\n",
       "          [ 0,  5,  5,  4, 12, 13,  5,  8,  7,  6, 17,  6,  4,  8, 11,  1,  2,  2,\n",
       "            2,  2],\n",
       "          [ 0,  8, 10,  4,  5, 13, 11,  3, 12,  4, 17,  4,  6, 11,  3,  6,  1,  2,\n",
       "            2,  2]], device='cuda:0'),\n",
       "  'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]],\n",
       "         device='cuda:0'),\n",
       "  'labels': tensor([1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1.],\n",
       "         device='cuda:0')})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%run 2.dataset.ipynb\n",
    "\n",
    "\n",
    "def f(data):\n",
    "    text = [i['text'] for i in data]\n",
    "    label = [i['label'] for i in data]\n",
    "\n",
    "    data = tokenizer(text, device=device)\n",
    "    data['labels'] = torch.FloatTensor(label).to(device)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "loader = get_loader(f, negative_label=True, with_answer=True)\n",
    "\n",
    "len(loader), next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "628012d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoXConfig {\n",
       "  \"_attn_implementation_autoset\": true,\n",
       "  \"architectures\": [\n",
       "    \"GPTNeoXForCausalLM\"\n",
       "  ],\n",
       "  \"attention_bias\": true,\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"classifier_dropout\": 0.1,\n",
       "  \"eos_token_id\": 1,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout\": 0.0,\n",
       "  \"hidden_size\": 768,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"LABEL_0\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"label2id\": {\n",
       "    \"LABEL_0\": 0\n",
       "  },\n",
       "  \"layer_norm_eps\": 1e-05,\n",
       "  \"max_position_embeddings\": 2048,\n",
       "  \"model_type\": \"gpt_neox\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 2,\n",
       "  \"partial_rotary_factor\": 0.25,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rope_theta\": 10000,\n",
       "  \"rotary_emb_base\": 10000,\n",
       "  \"rotary_pct\": 0.25,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"transformers_version\": \"4.48.0\",\n",
       "  \"use_cache\": true,\n",
       "  \"use_parallel_residual\": true,\n",
       "  \"vocab_size\": 19\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPTNeoXConfig, AutoModelForSequenceClassification\n",
    "\n",
    "model_critic = AutoModelForSequenceClassification.from_config(\n",
    "    GPTNeoXConfig(_attn_implementation_autoset=True,\n",
    "                  architectures=['GPTNeoXForCausalLM'],\n",
    "                  eos_token_id=tokenizer.eos_token_id,\n",
    "                  id2label={'0': 'LABEL_0'},\n",
    "                  label2id={'LABEL_0': 0},\n",
    "                  hidden_size=768,\n",
    "                  intermediate_size=3072,\n",
    "                  num_attention_heads=12,\n",
    "                  num_hidden_layers=12,\n",
    "                  vocab_size=len(tokenizer),\n",
    "                  pad_token_id=tokenizer.pad_token_id,\n",
    "                  num_labels=1)).to(device)\n",
    "\n",
    "model_critic.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77d000bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 125000 0.34103602170944214 0.5625\n",
      "1000 125000 0.20233052968978882 0.6875\n",
      "2000 125000 0.18607331812381744 0.75\n",
      "3000 125000 0.1396457999944687 0.6875\n",
      "4000 125000 0.10160748660564423 0.875\n",
      "5000 125000 0.13328039646148682 0.875\n",
      "6000 125000 0.126186341047287 0.8125\n",
      "7000 125000 0.11202022433280945 0.8125\n",
      "8000 125000 0.11786967515945435 0.6875\n",
      "9000 125000 0.1412677764892578 0.8125\n",
      "10000 125000 0.21582680940628052 0.625\n",
      "11000 125000 0.1226201206445694 0.75\n",
      "12000 125000 0.16518059372901917 0.8125\n",
      "13000 125000 0.05162348598241806 0.875\n",
      "14000 125000 0.08120504021644592 0.875\n",
      "15000 125000 0.18136709928512573 0.75\n",
      "16000 125000 0.11339117586612701 0.875\n",
      "17000 125000 0.06583108007907867 0.9375\n",
      "18000 125000 0.15697476267814636 0.75\n",
      "19000 125000 0.06264254450798035 0.875\n",
      "20000 125000 0.17975997924804688 0.8125\n",
      "21000 125000 0.160003662109375 0.8125\n",
      "22000 125000 0.0356559231877327 1.0\n",
      "23000 125000 0.1474703997373581 0.75\n",
      "24000 125000 0.09269499778747559 0.875\n",
      "25000 125000 0.1079915314912796 0.875\n",
      "26000 125000 0.10569532215595245 0.8125\n",
      "27000 125000 0.12434965372085571 0.75\n",
      "28000 125000 0.16958686709403992 0.75\n",
      "29000 125000 0.17517322301864624 0.8125\n",
      "30000 125000 0.011434270069003105 1.0\n",
      "31000 125000 0.07125996798276901 0.9375\n",
      "32000 125000 0.050001464784145355 0.9375\n",
      "33000 125000 0.07832012325525284 0.875\n",
      "34000 125000 0.01031532883644104 1.0\n",
      "35000 125000 0.013974699191749096 1.0\n",
      "36000 125000 0.017403986304998398 1.0\n",
      "37000 125000 0.09185623377561569 0.875\n",
      "38000 125000 0.0361965149641037 0.9375\n",
      "39000 125000 0.07205627113580704 0.875\n",
      "40000 125000 0.10553393512964249 0.875\n",
      "41000 125000 0.014480997808277607 1.0\n",
      "42000 125000 0.036580197513103485 0.9375\n",
      "43000 125000 0.0577450767159462 0.9375\n",
      "44000 125000 0.07488781958818436 0.9375\n",
      "45000 125000 0.04397330433130264 0.9375\n",
      "46000 125000 0.0268786009401083 1.0\n",
      "47000 125000 0.05285301059484482 0.9375\n",
      "48000 125000 0.037695325911045074 0.9375\n",
      "49000 125000 0.01849006488919258 1.0\n",
      "50000 125000 0.03238176554441452 0.9375\n",
      "51000 125000 0.13751991093158722 0.8125\n",
      "52000 125000 0.023662589490413666 0.9375\n",
      "53000 125000 0.015346541069447994 1.0\n",
      "54000 125000 0.011735275387763977 1.0\n",
      "55000 125000 0.0038564465939998627 1.0\n",
      "56000 125000 0.006699751131236553 1.0\n",
      "57000 125000 0.04706496745347977 0.9375\n",
      "58000 125000 0.11413106322288513 0.875\n",
      "59000 125000 0.04793538525700569 0.9375\n",
      "60000 125000 0.18162676692008972 0.6875\n",
      "61000 125000 0.005248093977570534 1.0\n",
      "62000 125000 0.02407909743487835 0.9375\n",
      "63000 125000 0.13415873050689697 0.8125\n",
      "64000 125000 0.053813353180885315 0.9375\n",
      "65000 125000 0.0012733053881675005 1.0\n",
      "66000 125000 0.10619313269853592 0.875\n",
      "67000 125000 0.11038613319396973 0.8125\n",
      "68000 125000 0.0340161956846714 0.9375\n",
      "69000 125000 0.01960224285721779 0.9375\n",
      "70000 125000 0.06598086655139923 0.875\n",
      "71000 125000 0.07646707445383072 0.9375\n",
      "72000 125000 0.04823431000113487 0.875\n",
      "73000 125000 0.04309805482625961 0.9375\n",
      "74000 125000 0.015512047335505486 1.0\n",
      "75000 125000 0.154154434800148 0.8125\n",
      "76000 125000 0.028000041842460632 0.9375\n",
      "77000 125000 0.10543615370988846 0.8125\n",
      "78000 125000 0.0717005804181099 0.875\n",
      "79000 125000 0.019336994737386703 1.0\n",
      "80000 125000 0.03504270315170288 0.9375\n",
      "81000 125000 0.014097660779953003 1.0\n",
      "82000 125000 0.02861781418323517 0.9375\n",
      "83000 125000 0.06302198022603989 0.9375\n",
      "84000 125000 0.004636083729565144 1.0\n",
      "85000 125000 0.013999680988490582 1.0\n",
      "86000 125000 0.0083216093480587 1.0\n",
      "87000 125000 0.009469139389693737 1.0\n",
      "88000 125000 0.13610005378723145 0.875\n",
      "89000 125000 0.01570879854261875 1.0\n",
      "90000 125000 0.009983609430491924 1.0\n",
      "91000 125000 0.013875658623874187 1.0\n",
      "92000 125000 0.04920873045921326 0.9375\n",
      "93000 125000 0.005253924988210201 1.0\n",
      "94000 125000 0.06984852999448776 0.875\n",
      "95000 125000 0.01961534656584263 0.9375\n",
      "96000 125000 0.0481501929461956 0.9375\n",
      "97000 125000 0.0012611259007826447 1.0\n",
      "98000 125000 0.0027819520328193903 1.0\n",
      "99000 125000 0.18432414531707764 0.75\n",
      "100000 125000 0.02882404252886772 0.9375\n",
      "101000 125000 0.006012442521750927 1.0\n",
      "102000 125000 0.12965521216392517 0.75\n",
      "103000 125000 0.011889026500284672 1.0\n",
      "104000 125000 0.03274106606841087 0.9375\n",
      "105000 125000 0.10177122056484222 0.875\n",
      "106000 125000 0.014879531227052212 1.0\n",
      "107000 125000 0.04344208166003227 0.9375\n",
      "108000 125000 0.0012636438477784395 1.0\n",
      "109000 125000 0.06021341308951378 0.9375\n",
      "110000 125000 0.04998840019106865 0.875\n",
      "111000 125000 0.03819071501493454 0.9375\n",
      "112000 125000 0.03882338106632233 0.9375\n",
      "113000 125000 0.01994975097477436 0.9375\n",
      "114000 125000 0.01949026621878147 1.0\n",
      "115000 125000 0.09660688787698746 0.875\n",
      "116000 125000 0.0024247157853096724 1.0\n",
      "117000 125000 0.013347632251679897 1.0\n",
      "118000 125000 0.034452587366104126 0.9375\n",
      "119000 125000 0.03712700307369232 0.9375\n",
      "120000 125000 0.07641930878162384 0.875\n",
      "121000 125000 0.004711255431175232 1.0\n",
      "122000 125000 0.05502620339393616 0.9375\n",
      "123000 125000 0.059567127376794815 0.9375\n",
      "124000 125000 0.005147913936525583 1.0\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model_critic.parameters(), lr=1e-5)\n",
    "\n",
    "for i, data in enumerate(loader):\n",
    "    out = model_critic(**data)\n",
    "    out.loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if i % 1000 == 0:\n",
    "        logits = (out.logits > 0.5).squeeze(1).long()\n",
    "        acc = (logits == data['labels'].long()).sum() / len(data['labels'])\n",
    "        print(i, len(loader), out.loss.item(), acc.item())\n",
    "\n",
    "model_critic.save_pretrained('model/critic')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pt2]",
   "language": "python",
   "name": "conda-env-pt2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
